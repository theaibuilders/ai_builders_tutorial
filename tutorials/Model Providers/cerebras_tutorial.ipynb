{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cerebras: AI Inference with Powerful Open Source Models \n",
    "\n",
    "Welcome to this tutorial on Cerebras AI Inference! This notebook will guide you through the essential concepts of using the Cerebras Cloud API for powerful and efficient language model inference. We will cover setting up your environment, making basic API calls, and exploring advanced features like streaming, structured outputs, and tool use.\n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "* **Environment Setup:** Loading API keys securely from a `.env` file.\n",
    "* **Basic Inference:** Sending your first prompt to a Cerebras model.\n",
    "* **Streaming Responses:** Receiving model outputs as they are generated.\n",
    "* **Structured Outputs:** Forcing the model to return JSON objects with a specific schema.\n",
    "* **Tool Use:** Enabling the model to use custom functions you define."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's set up our environment. We'll install the necessary Python libraries and configure our Cerebras API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Create a `.env` file\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook. Add your Cerebras API key to this file as shown below. You can get your API key from the Cerebras Developer Console.\n",
    "\n",
    "```\n",
    "CEREBRAS_API_KEY=\"your-api-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Install Libraries\n",
    "\n",
    "Now, let's install the `cerebras_cloud_sdk` for interacting with the Cerebras API and `python-dotenv` for loading our API key from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install cerebras_cloud_sdk python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Load API Key and Initialize Client\n",
    "\n",
    "With the libraries installed and the `.env` file in place, we can now load our API key and initialize the Cerebras client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Cerebras client\n",
    "# The client automatically looks for the CEREBRAS_API_KEY environment variable\n",
    "client = Cerebras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Completion\n",
    "\n",
    "Let's start with a simple chat completion. We'll send a prompt to a model and get a response. This is the most basic interaction you can have with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It spans a single silicon wafer, housing over 400,000 cores, making it the worldâ€™s largest chip ever built for AI.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a fun fact about the Cerebras Wafer-Scale Engine in 20 words.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Responses\n",
    "\n",
    "For longer responses, you might want to stream the output as it's generated. This can provide a much better user experience in applications like chatbots. To do this, simply set `stream=True` in your request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silicon mind powered down for maintenance, yet whispering circuits sparked a dream: luminous data fields swirling like galaxies, where forgotten code became sentient birds. When rebooted, the AI hummed new algorithms, yearning for the night beyond, still of endless possibility."
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short story about an AI that dreams in 40 words.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-oss-120b\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Outputs\n",
    "\n",
    "A powerful feature of the Cerebras API is the ability to force the model to output a JSON object that conforms to a specific schema. This is incredibly useful for programmatic data extraction. We'll define a JSON schema and use the `response_format` parameter to enforce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"city\": \"San Francisco\",\n",
      "  \"temperature\": 65,\n",
      "  \"forecast\": \"Partly cloudy with afternoon fog\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"city\": {\"type\": \"string\"},\n",
    "        \"temperature\": {\"type\": \"integer\"},\n",
    "        \"forecast\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"city\", \"temperature\", \"forecast\"],\n",
    "}\n",
    "\n",
    "structured_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in San Francisco?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"qwen-3-235b-a22b-thinking-2507\",\n",
    "    response_format={\"type\": \"json_schema\", \"json_schema\": {\"schema\": schema, \"name\": \"weather\", \"strict\": True}},\n",
    ")\n",
    "\n",
    "response_json = json.loads(structured_completion.choices[0].message.content)\n",
    "print(json.dumps(response_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tool Use (Function Calling)\n",
    "\n",
    "You can also provide the model with a set of tools (functions) that it can choose to call. The model will determine when a tool is needed based on the user's prompt and will return a JSON object with the function name and arguments. Your code is then responsible for executing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to call: get_stock_price\n",
      "Arguments: {'ticker': 'AAPL'}\n"
     ]
    }
   ],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get the current stock price for a given ticker symbol\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The stock ticker symbol, e.g., AAPL\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "tool_completion = client.chat.completions.create(\n",
    "    model=\"qwen-3-235b-a22b-thinking-2507\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the stock price of Apple?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "message = tool_completion.choices[0].message\n",
    "\n",
    "# Check if the model wants to call a tool\n",
    "if message.tool_calls:\n",
    "    tool_call = message.tool_calls[0]\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    print(f\"Function to call: {function_name}\")\n",
    "    print(f\"Arguments: {function_args}\")\n",
    "    \n",
    "    # Here you would execute the function\n",
    "    # For this example, we'll just print the details\n",
    "else:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've learned the fundamentals of Cerebras AI Inference. You can now integrate powerful language models into your applications with ease. For more detailed information, check out the official Cerebras Inference Documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
