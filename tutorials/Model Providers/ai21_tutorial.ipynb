{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI21 Studio Tutorial: Mastering the Chat Completions API\n",
    "\n",
    "Welcome to this advanced tutorial on AI21 Studio! This notebook demonstrates how to leverage the unified **Chat Completions API** for a wide range of tasks. Instead of using different APIs for different jobs, we'll show how to instruct a single model, **Jamba-mini**, to perform text generation, paraphrasing, summarization, and contextual Q&A through carefully crafted prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the necessary libraries. We'll then configure our `AI21Client`, which will automatically read your API key from a `.env` file for secure access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q ai21 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# The client automatically uses the AI21_API_KEY environment variable\n",
    "client = AI21Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. General Text Generation\n",
    "\n",
    "For general text generation, we provide the model with a user message containing our instruction. The model then generates a response in the role of an 'assistant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "The future of renewable energy is brighter than ever, with groundbreaking innovations and growing global investments driving its rapid expansion. Solar, wind, and battery technologies are becoming more efficient and cost-effective, making clean energy accessible to communities worldwide. As nations prioritize sustainability, renewable energy will play a pivotal role in combating climate change, creating jobs, and fostering energy independence. The transition to renewables is not just an environmental necessity but a powerful opportunity to build a cleaner, healthier, and more equitable future for generations to come.\n"
     ]
    }
   ],
   "source": [
    "def generate_text_with_chat(user_prompt):\n",
    "    \"\"\"Generates text using the Chat Completions API.\"\"\"\n",
    "    try:\n",
    "        messages = [ChatMessage(role=\"user\", content=user_prompt)]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a short, optimistic paragraph about the future of renewable energy.\"\n",
    "generated_text = generate_text_with_chat(prompt)\n",
    "print(f\"Generated Text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Paraphrasing as a Chat Task\n",
    "\n",
    "To paraphrase text, we instruct the model by assigning it a specific role using a **system message**. The system message guides the model's behavior, telling it to act as an expert editor. The user message then provides the text to be paraphrased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Paraphrased Text: The fast-moving brown fox leaps gracefully over the sluggish dog.\n"
     ]
    }
   ],
   "source": [
    "def paraphrase_text_with_chat(text_to_paraphrase):\n",
    "    \"\"\"Paraphrases text using an instruction-based chat call.\"\"\"\n",
    "    try:\n",
    "        system_prompt = \"You are an expert editor. Your task is to paraphrase the given text, ensuring the original meaning is perfectly preserved but the wording is different.\"\n",
    "        user_prompt = f\"Please paraphrase the following text: \\\"{text_to_paraphrase}\\\"\"\n",
    "        \n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=user_prompt)\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "original_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "paraphrased_text = paraphrase_text_with_chat(original_text)\n",
    "print(f\"Original Text: {original_text}\")\n",
    "print(f\"Paraphrased Text: {paraphrased_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarization as a Chat Task\n",
    "\n",
    "Similarly, we can perform summarization by defining the model's role in a system message and then passing the long text in the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "AI21 Labs, founded in 2017, is an AI company specializing in Natural Language Processing, aiming to build AI systems with advanced natural language understanding and generation capabilities. They have developed a family of large language models, including the Jurassic and Jamba series, which are among the largest and most sophisticated in the world, powering applications for text generation, summarization, and paraphrasing.\n"
     ]
    }
   ],
   "source": [
    "def summarize_text_with_chat(long_text):\n",
    "    \"\"\"Summarizes text using an instruction-based chat call.\"\"\"\n",
    "    try:\n",
    "        system_prompt = \"You are a helpful assistant that summarizes long texts into a single, concise paragraph.\"\n",
    "        user_prompt = f\"Please summarize the following text: \\n\\n{long_text}\"\n",
    "        \n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=user_prompt)\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "text_to_summarize = \"\"\"AI21 Labs is an AI company specializing in Natural Language Processing. Founded in 2017, its goal is to build AI systems with an unprecedented capacity to understand and generate natural language. They have developed a family of large language models, including the Jurassic and Jamba series, which are among the largest and most sophisticated in the world. These models power applications for text generation, summarization, and paraphrasing.\"\"\"\n",
    "summary = summarize_text_with_chat(text_to_summarize)\n",
    "print(f\"Summary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contextual Answering as a Chat Task\n",
    "\n",
    "Finally, we can build a Q&A system by providing the context and the question within the user prompt. The system prompt instructs the model to only use the provided context for its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The capital of France is Paris. Paris is known for its art, fashion, and culture.\n",
      "Question: What is the primary industry in Paris?\n",
      "Answer: The answer is not available in the provided text.\n"
     ]
    }
   ],
   "source": [
    "def get_contextual_answer_with_chat(context, question):\n",
    "    \"\"\"Gets a contextual answer using an instruction-based chat call.\"\"\"\n",
    "    try:\n",
    "        system_prompt = \"You are a question-answering assistant. You must answer the user's question based ONLY on the provided context. If the answer is not in the context, say 'The answer is not available in the provided text.'\"\n",
    "        user_prompt = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "        \n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=user_prompt)\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "qa_context = \"The capital of France is Paris. Paris is known for its art, fashion, and culture.\"\n",
    "qa_question = \"What is the primary industry in Paris?\"\n",
    "answer = get_contextual_answer_with_chat(qa_context, qa_question)\n",
    "print(f\"Context: {qa_context}\")\n",
    "print(f\"Question: {qa_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Next Steps\n",
    "\n",
    "In this notebook you saw how a single unified Chat Completions interface (with the `jamba-mini` model) can power multiple classic NLP capabilities simply by varying the conversation messages:\n",
    "\n",
    "**What we built**\n",
    "- General text generation with only a user message\n",
    "- Paraphrasing by assigning an explicit editing role via a system message\n",
    "- Summarization by constraining style and compression in the system prompt\n",
    "- Context‑bound Q&A that refuses to hallucinate beyond supplied context\n",
    "\n",
    "**Next Steps**\n",
    "- Wrap these functions in a lightweight FastAPI or Flask service.\n",
    "- Create a front‑end (Streamlit / Gradio) for interactive experimentation.\n",
    "- Expand to multi-turn conversations by appending prior `assistant` and `user` messages.\n",
    "- Explore larger AI21 models for higher quality where latency/token cost trade‑offs make sense.\n",
    "\n",
    "> With careful prompt design and a thin layer of helper functions, you can cover a large surface area of language tasks using one consistent chat endpoint. Happy building! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
