{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nosana: Using OpenAI GPT-OSS:20B with Ollama\n",
    "\n",
    "Welcome to this tutorial on running the latest OpenAI open-weight model, **gpt-oss:20b**, hosted on Nosana - a decentralized GPU network that allows for distributed model inference. This setup is fantastic for when you need more computational power than your local machine can offer, without the hassle of managing your own high-end hardware.\n",
    "\n",
    "> **Want to learn how to actually spin up a job on Nosana and retrieve the `base_url`?**\n",
    "> Check out these resources:\n",
    "> - [Nosana Dashboard](https://dashboard.nosana.com)\n",
    "> - [Nosana CLI on GitHub](https://github.com/nosana-ci/nosana-cli/)\n",
    "\n",
    "We'll cover:\n",
    "* Setting up your environment.\n",
    "* Connecting to a remote Nosana Base URL.\n",
    "* Pulling and interacting with the `gpt-oss:20b` model.\n",
    "* Basic text generation, streaming, and chat.\n",
    "* A simple example of function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll use `ollama` to communicate with the Ollama server and `python-dotenv` to manage our environment variables securely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "To connect to our remote server, we need to tell the Ollama client its address. We'll store this in a `.env` file to keep our configuration clean and separate from our code.\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook and add your remote server URL to it. If you're using Nosana, this will be your unique `NOSANA_BASE_URL`.\n",
    "\n",
    "**Your `.env` file should look like this:**\n",
    "```\n",
    "OLLAMA_HOST=your_nosana_base_url_here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Configuration and Connecting\n",
    "\n",
    "Now, let's load the environment variable from our `.env` file. The `ollama-python` library is smart and will automatically use the `OLLAMA_HOST` variable we've set. For clarity, we will also show how to create a client and explicitly pass the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to remote Ollama server at: https://4w9w89qshprb...node.k8s.prd.nos.ci/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the remote server URL from environment variables\n",
    "ollama_host = os.getenv(\"NOSANA_BASE_URL\")\n",
    "\n",
    "def short_link(link):\n",
    "    if link and len(link) > 10:\n",
    "        return link[:20] + '...' + link[-20:]\n",
    "    return link\n",
    "\n",
    "if not ollama_host:\n",
    "    print(\"OLLAMA_HOST environment variable not found!\")\n",
    "    print(\"Please create a .env file and add your remote server URL.\")\n",
    "else:\n",
    "    print(f\"Connecting to remote Ollama server at: {short_link(ollama_host)}\")\n",
    "\n",
    "# You can create a client explicitly, which is good practice\n",
    "client = ollama.Client(host=ollama_host)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interacting with the Model\n",
    "\n",
    "With our connection established, we can start interacting with the `gpt-oss:20b` model. If the model isn't already available on the remote server, Ollama will download it automatically on the first run. You can also explicitly pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pulling the 'gpt-oss:20b' model. This may take a while..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Model pulled successfully!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'gpt-oss:20b'\n",
    "\n",
    "try:\n",
    "    display(Markdown(f\"Pulling the '{model_name}' model. This may take a while...\"))\n",
    "    client.pull(model_name)\n",
    "    display(Markdown(\"Model pulled successfully!\"))\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"Error: {e}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Generation\n",
    "\n",
    "Let's start with a simple text generation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model is an AI system trained on vast text data that learns statistical patterns of language so it can generate, translate, or understand text in a way that mimics human style.\n"
     ]
    }
   ],
   "source": [
    "response = client.generate(\n",
    "    model=model_name,\n",
    "    prompt='Explain the concept of a Large Language Model in one sentence.'\n",
    ")\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "For more interactive applications, you can stream the response as it's being generated. This is great for showing a real-time typing effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steel heart, dormant in the workshop, scanned old vinyl. A crackling needle whispered rhythm. The robot‚Äôs circuits sparked, translating harmonies into code. With each chord, gears wavered, emotions blooming. It recorded melodies, breathing life into metal. Music, the universe‚Äôs pulse, filled his void, and he sang for eternal resonance always."
     ]
    }
   ],
   "source": [
    "stream = client.generate(\n",
    "    model=model_name,\n",
    "    prompt='Write a short story about a robot who discovers music in 50 words.',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['response'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Interface\n",
    "\n",
    "The `chat` method is designed for conversational interactions, where the model remembers the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Python remains the cornerstone of AI development, offering extensive libraries (TensorFlow, PyTorch, Scikit‚Äëlearn), a clear syntax, and a massive community. Its rapid prototyping, readability, and cross‚Äëplatform compatibility make researchers and engineers quickly build, test, and deploy models, keeping AI accessible to all developers in industry and academia alike and beyond."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is the most important programming language for AI development? Explain in 50 words.'\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_response = client.chat(model=model_name, messages=messages)\n",
    "display(Markdown(chat_response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced: Function Calling\n",
    "\n",
    "The `gpt-oss` models have strong capabilities for **function calling** (or tool use). This allows the model to request the invocation of a function you've defined in your code to get external information or perform an action.\n",
    "\n",
    "Here's a simple example where we define a tool to get the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore‚Äôs weather is typically hot and humid year‚Äêround. Right now the temperature is about **27‚ÄØ¬∞C** (‚âà80‚ÄØ¬∞F). It‚Äôs in the range of 25‚Äì31‚ÄØ¬∞C most days, with high humidity and a chance of brief showers, especially during the monsoon seasons.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def get_current_weather(city: str):\n",
    "    \"\"\"Get the current weather in a given city using Open-Meteo API\"\"\"\n",
    "    # Geocoding to get latitude and longitude\n",
    "    geo_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city}&count=1\"\n",
    "    geo_resp = requests.get(geo_url)\n",
    "    geo_data = geo_resp.json()\n",
    "    if not geo_data.get(\"results\"):\n",
    "        return json.dumps({\"city\": city, \"temperature\": \"unknown\", \"unit\": \"celsius\"})\n",
    "    lat = geo_data[\"results\"][0][\"latitude\"]\n",
    "    lon = geo_data[\"results\"][0][\"longitude\"]\n",
    "    # Get current weather\n",
    "    weather_url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
    "    weather_resp = requests.get(weather_url)\n",
    "    weather_data = weather_resp.json()\n",
    "    temp = weather_data.get(\"current_weather\", {}).get(\"temperature\")\n",
    "    if temp is None:\n",
    "        return json.dumps({\"city\": city, \"temperature\": \"unknown\", \"unit\": \"celsius\"})\n",
    "    return json.dumps({\"city\": city, \"temperature\": temp, \"unit\": \"celsius\"})\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'get_current_weather',\n",
    "            'description': 'Get the current weather in a given city',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'city': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The city, e.g., San Francisco',\n",
    "                    },\n",
    "                },\n",
    "                'required': ['city'],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is the weather like in Singapore?'}]\n",
    "\n",
    "# First, let the model decide which tool to call\n",
    "response = client.chat(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "messages.append(response['message'])\n",
    "\n",
    "# Then, execute the tool and send the result back to the model\n",
    "if response['message'].get('tool_calls'):\n",
    "    tool_call = response['message']['tool_calls'][0]\n",
    "    function_name = tool_call['function']['name']\n",
    "    function_args = tool_call['function']['arguments']  # Already a dict\n",
    "    \n",
    "    # Call the function\n",
    "    function_response = get_current_weather(city=function_args.get('city'))\n",
    "    \n",
    "    messages.append(\n",
    "        {\n",
    "            'role': 'tool',\n",
    "            'content': function_response,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get the final response from the model\n",
    "    final_response = client.chat(model=model_name, messages=messages)\n",
    "    print(final_response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! üéâ You've successfully connected to a remote Ollama server, interacted with the `gpt-oss:20b` model, and even explored its function-calling capabilities. \n",
    "\n",
    "This remote setup unlocks the ability to work with powerful models from anywhere, without needing a supercomputer at your desk. From here, you can build complex applications, experiment with different models, or fine-tune models for your specific needs.\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to explore more models and power your AI apps?**\n",
    "\n",
    "üëâ [Visit Nosana.com to discover more models and supercharge your AI projects!](https://nosana.com/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
