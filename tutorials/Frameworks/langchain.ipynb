{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial: From Fundamentals to Advanced RAG\n",
    "\n",
    "This tutorial provides a comprehensive overview of LangChain, covering the essential concepts from basic setup to building a sophisticated Retrieval-Augmented Generation (RAG) system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the entire lifecycle of LLM application development, including development, productionization, and deployment.\n",
    "\n",
    "### Core Concepts:\n",
    "\n",
    "- **Chains & LCEL:** Chains are sequences of operations (like prompts, models, and parsers) that can be composed using the LangChain Expression Language (LCEL) and the `|` (pipe) operator for flexible, declarative workflows.\n",
    "- **Components:** LangChain provides modular building blocks:\n",
    "  - **Models:** Interfaces for LLMs, chat, and embedding models from providers like OpenAI, Anthropic, and Google.\n",
    "  - **Prompts:** Templates for dynamic, parameterized prompt construction.\n",
    "  - **Output Parsers:** Convert LLM outputs into structured formats (strings, JSON, Pydantic models, etc.).\n",
    "  - **Memory:** Tools for maintaining state across chain runs, supporting context retention.\n",
    "  - **Tools:** External functions/APIs that can be called by the LLM during execution.\n",
    "- **Retrievers & RAG:** Retrievers fetch relevant data from sources (like vector stores or databases) to augment LLM responses. Retrieval-Augmented Generation (RAG) combines retrievers and LLMs for up-to-date, domain-specific answers.\n",
    "- **Document Loaders & Text Splitters:** Utilities for ingesting and chunking data from various sources for use in retrieval pipelines.\n",
    "\n",
    "### Installation\n",
    "\n",
    "First, let's install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install core LangChain and provider-specific packages\n",
    "# !pip install langchain langchain-core langchain-community langchain-openai langchain-chroma faiss-cpu pypdf sentence-transformers tiktoken -q\n",
    "\n",
    "# Uncomemnt to install environment management and web request libraries\n",
    "# !pip install python-dotenv requests beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure your API keys. It's recommended to use environment variables for security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"‚úÖ API keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Your First Chain with LCEL\n",
    "\n",
    "A \"chain\" in LangChain is a sequence of calls to components. We'll use the LangChain Expression Language (LCEL) to build a simple chain.\n",
    "\n",
    "### Initialize the Model\n",
    "\n",
    "We'll start by initializing a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized model: ChatOpenAI\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize a chat model from OpenAI\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
    "\n",
    "print(f\"‚úÖ Initialized model: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Prompt Templates\n",
    "\n",
    "Prompt templates allow you to create reusable and parameterized prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum entanglement is a phenomenon in quantum physics where two or more particles become linked in such a way that the state of one particle instantly influences the state of the other, no matter how far apart they are. This means that if you measure one entangled particle and find its property (like spin or polarization), you can immediately know the corresponding property of the other particle, even if it is light-years away. It challenges our classical understanding of separateness and locality, leading to intriguing implications for information and communication.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in {expertise}.\"),\n",
    "    (\"human\", \"Explain {topic} in a simple and concise way.\")\n",
    "])\n",
    "\n",
    "# Create a simple chain using LCEL\n",
    "# The chain will:\n",
    "# 1. Take user input for 'expertise' and 'topic'.\n",
    "# 2. Format the prompt using the prompt_template.\n",
    "# 3. Pass the formatted prompt to the model.\n",
    "# 4. Parse the model's output to a string.\n",
    "simple_chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "response = simple_chain.invoke({\n",
    "    \"expertise\": \"physics\",\n",
    "    \"topic\": \"quantum entanglement\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "For a better user experience, you can stream the model's response as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming response:\n",
      "The Maillard reaction is a chemical process that occurs when proteins and sugars in food are heated together, resulting in browning and the development of complex flavors and aromas. This reaction typically happens at high temperatures during cooking, such as roasting or grilling. It‚Äôs responsible for the delicious crust on bread, the golden color of seared meats, and the rich flavors in many cooked foods. Essentially, it's what makes food taste and look more appealing when cooked."
     ]
    }
   ],
   "source": [
    "print(\"üåä Streaming response:\")\n",
    "for chunk in simple_chain.stream({\n",
    "    \"expertise\": \"culinary arts\",\n",
    "    \"topic\": \"the Maillard reaction\"\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Output and Advanced Chains\n",
    "\n",
    "LangChain can parse model outputs into structured formats and build more complex, conditional chains.\n",
    "\n",
    "### Pydantic Output Parser\n",
    "\n",
    "Use Pydantic models to define the desired output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe for: Simple Scrambled Eggs\n",
      "\n",
      "Ingredients:\n",
      "- 2 large eggs\n",
      "- Salt to taste\n",
      "- Pepper to taste\n",
      "- 1 tablespoon butter\n",
      "- Fresh herbs (optional, for garnish)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define a Pydantic model for structured output\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"The name of the recipe\")\n",
    "    ingredients: List[str] = Field(description=\"A list of ingredients\")\n",
    "    steps: List[str] = Field(description=\"The steps to prepare the recipe\")\n",
    "\n",
    "# Create a Pydantic output parser\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# Create a prompt that includes format instructions\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class chef. Generate a recipe based on the user's request and format it as requested.\"),\n",
    "    (\"human\", \"I want a simple recipe for {dish}.\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Create the structured output chain\n",
    "structured_chain = structured_prompt | model | pydantic_parser\n",
    "\n",
    "# Invoke the chain\n",
    "recipe_request = {\n",
    "    \"dish\": \"scrambled eggs\",\n",
    "    \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "}\n",
    "recipe_output = structured_chain.invoke(recipe_request)\n",
    "\n",
    "print(f\"Recipe for: {recipe_output.name}\")\n",
    "print(\"\\nIngredients:\")\n",
    "for ingredient in recipe_output.ingredients:\n",
    "    print(f\"- {ingredient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Chains with `RunnableBranch`**\n",
    "\n",
    "Create dynamic chains that change their behavior based on input conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beginner Explanation ---\n",
      "A black hole is a region in space where gravity is so strong that nothing, not even light, can escape from it. They form when massive stars collapse under their own gravity after exhausting their fuel. Imagine a vacuum cleaner that pulls everything in; that's a black hole, pulling in matter and energy from its surroundings.\n",
      "\n",
      "--- Expert Explanation ---\n",
      "Black holes are regions in spacetime where gravity is so intense that nothing, not even light, can escape. Formed from collapsing massive stars, they are characterized by an event horizon‚Äîthe boundary beyond which escape is impossible. Their mass, charge, and angular momentum define their properties, with singularities at their cores representing points of infinite density and spacetime curvature.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Define different prompts for different levels\n",
    "beginner_prompt = ChatPromptTemplate.from_template(\"Explain {topic} to a complete beginner within 60 words.\")\n",
    "expert_prompt = ChatPromptTemplate.from_template(\"Provide a detailed, technical explanation of {topic} within 60 words.\")\n",
    "\n",
    "# Create a conditional chain using RunnableBranch\n",
    "# This chain checks the 'level' input and routes to the appropriate prompt\n",
    "conditional_chain = (\n",
    "    RunnableBranch(\n",
    "        (lambda x: x.get(\"level\") == \"expert\", expert_prompt),\n",
    "        beginner_prompt  # Default prompt\n",
    "    )\n",
    "    | model.bind(max_tokens=100)\n",
    "    | StrOutputParser()\n",
    " )\n",
    "\n",
    "# Test the beginner path\n",
    "beginner_response = conditional_chain.invoke({\"topic\": \"black holes\", \"level\": \"beginner\"})\n",
    "print(\"--- Beginner Explanation ---\")\n",
    "print(beginner_response)\n",
    "\n",
    "# Test the expert path\n",
    "expert_response = conditional_chain.invoke({\"topic\": \"black holes\", \"level\": \"expert\"})\n",
    "print(\"\\n--- Expert Explanation ---\")\n",
    "print(expert_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "RAG connects your LLM to external data, allowing it to answer questions about information it wasn't trained on.\n",
    "\n",
    "**Step 1: Document Loading and Splitting**\n",
    "\n",
    "Load data from a source (like a website) and split it into smaller chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents and split them into 3 chunks.\n",
      "\n",
      "üìñ Preview of chunked text:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content: Page Not Found | ü¶úÔ∏èüîó LangChain...\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/modules/model_io/prompts/', 'title': 'Page Not Found | ü¶úÔ∏èüîó LangChain', 'language': 'en'}\n",
      "Full length: 30 characters\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content: Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain Hu...\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/modules/model_io/prompts/', 'title': 'Page Not Found | ü¶úÔ∏èüîó LangChain', 'language': 'en'}\n",
      "Full length: 427 characters\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content: them know their link is broken.CommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc....\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/modules/model_io/prompts/', 'title': 'Page Not Found | ü¶úÔ∏èüîó LangChain', 'language': 'en'}\n",
      "Full length: 151 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents from a web page\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/modules/model_io/prompts/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Split the documents into chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents and split them into {len(splits)} chunks.\")\n",
    "\n",
    "# Preview the first few chunks\n",
    "print(\"\\nüìñ Preview of chunked text:\")\n",
    "for i, chunk in enumerate(splits[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")  # Show first 200 characters\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Embeddings and Vector Stores**\n",
    "\n",
    "Convert the text chunks into numerical representations (embeddings) and store them in a vector database for efficient searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Embedding Preview:\n",
      "Sample text: 'LangChain is a framework for developing applications powered by language models.'\n",
      "Embedding dimension: 1536\n",
      "Embedding type: <class 'list'>\n",
      "First 10 values: [-0.03170353174209595, 0.009393854066729546, 0.045744992792606354, -0.013159518130123615, 0.040198035538196564, 0.0005537528777495027, -0.025970900431275368, 0.025228211656212807, -0.03571869060397148, 0.020191853865981102]\n",
      "Embedding range: [-0.0777, 0.0940]\n",
      "Embedding norm: 1.0000\n",
      "\n",
      "‚úÖ Vector store created with 3 document chunks.\n",
      "Each chunk has been converted to a 1536-dimensional embedding vector.\n",
      "\n",
      "‚úÖ Vector store created with 3 document chunks.\n",
      "Each chunk has been converted to a 1536-dimensional embedding vector.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Preview embeddings - let's see what embeddings look like\n",
    "sample_text = \"LangChain is a framework for developing applications powered by language models.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(\"üìä Embedding Preview:\")\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"Embedding type: {type(sample_embedding)}\")\n",
    "print(f\"First 10 values: {sample_embedding[:10]}\")\n",
    "print(f\"Embedding range: [{min(sample_embedding):.4f}, {max(sample_embedding):.4f}]\")\n",
    "print(f\"Embedding norm: {np.linalg.norm(sample_embedding):.4f}\")\n",
    "\n",
    "# Create a Chroma vector store from the document splits\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created with {len(splits)} document chunks.\")\n",
    "print(\"Each chunk has been converted to a {}-dimensional embedding vector.\".format(len(sample_embedding)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Creating a Retriever**\n",
    "\n",
    "A retriever is responsible for finding the most relevant document chunks based on a user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for the query: 'What are prompt templates?'\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test the retriever\n",
    "query = \"What are prompt templates?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for the query: '{query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Building the RAG Chain**\n",
    "\n",
    "Now, let's combine the retriever with a prompt and the LLM to create a complete RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: How can I use few-shot examples in my prompts?\n",
      "üéØ Answer: Few-shot examples can be effectively used in your prompts to guide the model's responses by providing context and specific examples of the desired output. Here‚Äôs how you can do it:\n",
      "\n",
      "1. **Define the Task Clearly**: Start by clearly stating what you want the model to do. This sets the stage for the examples you will provide.\n",
      "\n",
      "2. **Provide Examples**: Include a few examples that illustrate the input-output pairs. Each example should show a clear relationship between the input and the expected output. Typically, 2-5 examples are sufficient for few-shot prompting.\n",
      "\n",
      "3. **Use a Consistent Format**: Make sure the format of your examples is consistent. This helps the model understand the pattern it should follow.\n",
      "\n",
      "4. **End with a New Input**: After providing the examples, include the new input you want the model to respond to. This should be in the same format as the examples.\n",
      "\n",
      "5. **Keep It Concise**: While examples are important, keep your prompt concise to avoid overwhelming the model with too much information.\n",
      "\n",
      "### Example Prompt Structure\n",
      "\n",
      "```\n",
      "Task: Translate the following English sentences into Spanish.\n",
      "\n",
      "Example 1:\n",
      "Input: \"Hello, how are you?\"\n",
      "Output: \"Hola, ¬øc√≥mo est√°s?\"\n",
      "\n",
      "Example 2:\n",
      "Input: \"What is your name?\"\n",
      "Output: \"¬øCu√°l es tu nombre?\"\n",
      "\n",
      "Example 3:\n",
      "Input: \"I love programming.\"\n",
      "Output: \"Me encanta programar.\"\n",
      "\n",
      "Now, translate this sentence:\n",
      "Input: \"Where is the nearest restaurant?\"\n",
      "Output:\n",
      "```\n",
      "\n",
      "In this structure, you clearly define the task, provide a few relevant examples, and then present a new input for translation, guiding the model towards producing the expected output.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the following context to answer the user's question.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Helper function to format the retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the RAG chain\n",
    "rag_question = \"How can I use few-shot examples in my prompts?\"\n",
    "rag_answer = rag_chain.invoke(rag_question)\n",
    "\n",
    "print(f\"\\n‚ùì Question: {rag_question}\")\n",
    "print(f\"üéØ Answer: {rag_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Learned\n",
    "\n",
    "### üéØ Core Concepts Mastered\n",
    "\n",
    "**LangChain Expression Language (LCEL)**\n",
    "- Built chains using the powerful `|` (pipe) operator\n",
    "- Composed multiple components into seamless workflows\n",
    "- Learned declarative programming for LLM applications\n",
    "\n",
    "**Essential Components**\n",
    "- **Models**: Initialized and configured chat models with specific parameters\n",
    "- **Prompts**: Created reusable templates with dynamic variables\n",
    "- **Output Parsers**: Structured LLM outputs into usable formats (strings, Pydantic models)\n",
    "- **Retrievers**: Built document search systems for external knowledge\n",
    "\n",
    "### üõ†Ô∏è Practical Skills Developed\n",
    "\n",
    "**Chain Building Techniques**\n",
    "- Simple linear chains for basic text generation\n",
    "- Structured output chains with Pydantic validation\n",
    "- Conditional chains that adapt behavior based on input\n",
    "- RAG chains that combine retrieval with generation\n",
    "\n",
    "**Document Processing Pipeline**\n",
    "- Web scraping and content extraction\n",
    "- Text chunking strategies for optimal retrieval\n",
    "- Vector embeddings and similarity search\n",
    "- Knowledge base creation and querying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: Every complex AI application starts with the simple patterns you've learned today. Keep building, keep learning, and keep pushing the boundaries of what's possible with LangChain! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
