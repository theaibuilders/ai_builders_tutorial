{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM Tutorial: Unified API for 100+ LLMs\n",
    "\n",
    "LiteLLM is a Python library that provides a **unified interface** for calling 100+ Language Model APIs using the OpenAI format. It simplifies integration across providers like OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more.\n",
    "\n",
    "## Key Features\n",
    "- **Unified API**: Same interface for all providers\n",
    "- **Load Balancing**: Router with retry/fallback logic\n",
    "- **Cost Tracking**: Built-in spend monitoring\n",
    "- **Streaming Support**: Real-time response streaming\n",
    "- **Error Handling**: Consistent exception handling\n",
    "- **Async Support**: Full async/await compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install LiteLLM and python-dotenv with compatible versions\n",
    "#!pip install \"litellm>=1.0.0\" \"python-dotenv>=1.0.0\" \"pydantic>=2.0.0,<3.0.0\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import litellm\n",
    "from litellm import completion, Router\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys from .env file and create explicit variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Completion Calls\n",
    "\n",
    "LiteLLM uses the **same `completion()` function** for all providers. Just change the model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# OpenAI GPT-4\n",
    "if openai_api_key:\n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "    )\n",
    "    print(\"OpenAI:\", response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"âŒ OpenAI API key not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Provider Examples\n",
    "\n",
    "LiteLLM supports **100+ providers** with consistent formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ðŸ”„ Testing Multiple LLM Providers\n",
       "\n",
       "#### ðŸŸ¢ OpenAI GPT-4o-mini\n",
       "**Response:** Artificial Intelligence (AI) is the simulation of human intelligence in machines, enabling them to perform tasks like learning, reasoning, problem-solving, and understanding language, aimed at mimicking cognitive functions.\n",
       "\n",
       "#### ðŸŸ¢ Anthropic Claude-3-Haiku\n",
       "**Response:** AI (Artificial Intelligence) is the simulation of human intelligence in machines, capable of performing tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
       "\n",
       "#### ðŸŸ¢ HuggingFace SmolLM3-3B\n",
       "**Response:** <think>\n",
       "Okay, the user wants an explanation of AI in 30 words. Let me start by defining what AI is. It's short for Artificial Intelligence. I need to mention that it's the simulation of human intelligence in machines. But how to condense that into 30 words?\n",
       "\n",
       "First, the key points: AI is a field of computer science, involves machines mimicking human thought processes. Applications like language translation, decision-making, problem-solving. Maybe mention machine learning and neural networks as technologies used in AI.\n",
       "\n",
       "Wait, the user might not need the technical jargon. So keep it simple. Maybe start with \"Artificial Intelligence (AI) is technology that enables machines to perform tasks requiring human-like intelligence.\" Then add applications: \"like speech recognition, decision-making, and problem-solving.\" But that's 10 words. Need to expand to 30.\n",
       "\n",
       "How about: \"Artificial Intelligence (AI) is technology that allows machines to think and learn like humans, using algorithms and data to process information, recognize patterns, and make decisions. It powers applications in healthcare, finance, and customer service.\" That's 23 words. Still need 7 more. Maybe add something about machine learning and neural networks. \"AI uses machine learning and neural networks to analyze data, improve performance over time, and automate complex tasks.\" Now it's 30 words. Let me check the count again. \"Artificial Intelligence (AI) is technology that allows machines to think and learn like humans, using algorithms and data to process information, recognize patterns, and make decisions. It powers applications in healthcare, finance, and customer service, utilizing machine learning and neural networks to analyze data, improve performance over time, and automate complex tasks.\" Yes, that's 30 words. It covers the core aspects: definition, how it works, applications, and key technologies. I think that works.\n",
       "</think>\n",
       "\n",
       "Artificial Intelligence (AI) is technology that enables machines to think and learn like humans, using algorithms and data to process information, recognize patterns, and make decisions. It powers applications in healthcare, finance, and customer service, utilizing machine learning and neural networks to analyze data, improve performance over time, and automate complex tasks. (30 words)\n",
       "\n",
       "#### ðŸŸ¢ Ollama (Local) - Gemma 2B\n",
       "**Response:** Artificial Intelligence (AI) is a computer system that can perform tasks that require human intelligence, such as learning, problem-solving, and decision-making.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "\n",
    "# Suppress Pydantic warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Expected.*fields but got.*\")\n",
    "\n",
    "user_message = [{\"role\": \"user\", \"content\": \"Exlaine what is AI in 30 words\"}]\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "results.append(\"### ðŸ”„ Testing Multiple LLM Providers\\n\")\n",
    "\n",
    "# OpenAI GPT-4\n",
    "def get_openai_result():\n",
    "    if openai_api_key:\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=user_message\n",
    "            )\n",
    "            return f\"#### ðŸŸ¢ OpenAI GPT-4o-mini\\n**Response:** {response.choices[0].message.content}\\n\"\n",
    "        except Exception as e:\n",
    "            return f\"#### ðŸ”´ OpenAI Error\\n**Error Type:** {type(e).__name__}\\n**Details:** {str(e)}\\n\"\n",
    "    else:\n",
    "        return \"#### ðŸŸ¡ OpenAI\\n**Status:** API key not found\\n\"\n",
    "results.append(get_openai_result())\n",
    "\n",
    "# Anthropic Claude\n",
    "def get_claude_result():\n",
    "    if anthropic_api_key:\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=\"claude-3-haiku-20240307\",\n",
    "                messages=user_message\n",
    "            )\n",
    "            return f\"#### ðŸŸ¢ Anthropic Claude-3-Haiku\\n**Response:** {response.choices[0].message.content}\\n\"\n",
    "        except Exception as e:\n",
    "            return f\"#### ðŸ”´ Claude Error\\n**Error Type:** {type(e).__name__}\\n**Details:** {str(e)}\\n\"\n",
    "    else:\n",
    "        return \"## ðŸŸ¡ Anthropic Claude\\n**Status:** API key not found\\n\"\n",
    "results.append(get_claude_result())\n",
    "\n",
    "# HuggingFace\n",
    "def get_hf_result():\n",
    "    if huggingface_api_key:\n",
    "        try:\n",
    "            hf_response = completion(\n",
    "                model=\"huggingface/HuggingFaceTB/SmolLM3-3B\",\n",
    "                messages=user_message,\n",
    "                api_key=huggingface_api_key\n",
    "            )\n",
    "            return f\"#### ðŸŸ¢ HuggingFace SmolLM3-3B\\n**Response:** {hf_response.choices[0].message.content}\\n\"\n",
    "        except Exception as e:\n",
    "            return f\"#### ðŸ”´ HuggingFace Error\\n**Error Type:** {type(e).__name__}\\n**Details:** {str(e)}\\n\"\n",
    "    else:\n",
    "        return \"#### ðŸŸ¡ HuggingFace\\n**Status:** API key not found\\n\"\n",
    "results.append(get_hf_result())\n",
    "\n",
    "# Local Ollama (no API key needed)\n",
    "def get_ollama_result():\n",
    "    try:\n",
    "        ollama_response = completion(\n",
    "            model=\"ollama/gemma:2b\",\n",
    "            messages=user_message,\n",
    "            api_base=\"http://localhost:11434\"\n",
    "        )\n",
    "        return f\"#### ðŸŸ¢ Ollama (Local) - Gemma 2B\\n**Response:** {ollama_response.choices[0].message.content}\\n\"\n",
    "    except Exception as e:\n",
    "        return f\"#### ðŸ”´ Ollama (Local)\\n**Status:** Not available\\n**Details:** {str(e)}\\n\"\n",
    "results.append(get_ollama_result())\n",
    "\n",
    "# Display all results as markdown\n",
    "markdown_output = \"\\n\".join(results)\n",
    "display(Markdown(markdown_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Support\n",
    "\n",
    "Get **real-time streaming responses** by setting `stream=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "AI,, a marvel a marvel of modern of modern day,\n",
      " day,\n",
      "EndlessEndless possibilities in possibilities in its way. its way.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming response\n",
    "response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about AI within 20 words\"}],\n",
    "    stream=True,\n",
    "    max_tokens=30\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Router: Load Balancing & Fallbacks\n",
    "\n",
    "The **Router** enables load balancing, retries, and fallbacks across multiple deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router response: Hello, how can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Configure multiple model deployments\n",
    "model_list = [\n",
    "    {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"azure/gpt-35-turbo\",\n",
    "            \"api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "            \"api_base\": os.getenv(\"AZURE_API_BASE\"),\n",
    "            \"rpm\": 100  # requests per minute\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"rpm\": 200\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create router with fallbacks\n",
    "router = Router(\n",
    "    model_list=model_list,\n",
    "    num_retries=3,\n",
    "    timeout=30,\n",
    "    fallbacks=[{\"gpt-3.5-turbo\": [\"gpt-4o-mini\"]}]  # fallback to GPT-4o-mini if needed\n",
    ")\n",
    "\n",
    "# Make request through router\n",
    "response = router.completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello from router!\"}]\n",
    ")\n",
    "print(\"Router response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling & Retries\n",
    "\n",
    "LiteLLM standardizes **error handling** across all providers and supports automatic retries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Error: name 'completion' is not defined\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAIError\n",
    "\n",
    "try:\n",
    "    # This will retry 3 times on failure\n",
    "    response = completion(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Test message\"}],\n",
    "        num_retries=3\n",
    "    )\n",
    "    print(\"Success:\", response.choices[0].message.content)\n",
    "    \n",
    "except OpenAIError as e:\n",
    "    print(f\"LiteLLM Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Async Support\n",
    "\n",
    "LiteLLM supports **async/await** for concurrent operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running async example...\n",
      "Async result: Hello! How can I assist you today?\n",
      "\n",
      "Running multiple async calls...\n",
      "Async result: Hello! How can I assist you today?\n",
      "\n",
      "Running multiple async calls...\n",
      "Response 1: AI, which stands for artificial intelligence, refe...\n",
      "Response 2: Machine learning is a subset of artificial intelli...\n",
      "Response 3: Deep learning is a subset of machine learning that...\n",
      "Response 1: AI, which stands for artificial intelligence, refe...\n",
      "Response 2: Machine learning is a subset of artificial intelli...\n",
      "Response 3: Deep learning is a subset of machine learning that...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import warnings\n",
    "from litellm import acompletion\n",
    "\n",
    "# Suppress Pydantic warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "async def async_completion_example():\n",
    "    \"\"\"Example of async completion\"\"\"\n",
    "    response = await acompletion(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello async world!\"}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def multiple_async_calls():\n",
    "    \"\"\"Make multiple concurrent API calls\"\"\"\n",
    "    tasks = [\n",
    "        acompletion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What is {topic}?\"}]\n",
    "        )\n",
    "        for topic in [\"AI\", \"Machine Learning\", \"Deep Learning\"]\n",
    "    ]\n",
    "    \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"Response {i+1}: {response.choices[0].message.content[:50]}...\")\n",
    "\n",
    "# Fix for Jupyter notebooks - use await instead of asyncio.run()\n",
    "print(\"Running async example...\")\n",
    "result = await async_completion_example()\n",
    "print(\"Async result:\", result)\n",
    "\n",
    "print(\"\\nRunning multiple async calls...\")\n",
    "await multiple_async_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**LiteLLM** simplifies LLM integration by providing:\n",
    "\n",
    "1. **Unified API** - Same interface for 100+ providers\n",
    "2. **Reliability** - Built-in retries, fallbacks, and load balancing\n",
    "3. **Observability** - Cost tracking, logging, and monitoring\n",
    "4. **Performance** - Streaming and async support\n",
    "5. **Production-ready** - Error handling and advanced routing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
