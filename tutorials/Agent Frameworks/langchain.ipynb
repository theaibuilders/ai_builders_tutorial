{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangChain Tutorial: From Fundamentals to Advanced RAG**\n",
    "\n",
    "This tutorial provides a comprehensive overview of LangChain, covering the essential concepts from basic setup to building a sophisticated Retrieval-Augmented Generation (RAG) system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction and Setup**\n",
    "\n",
    "**What is LangChain?**\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the entire lifecycle of LLM application development, including development, productionization, and deployment.\n",
    "\n",
    "**Core Concepts:**\n",
    "\n",
    "* **LangChain Expression Language (LCEL):** A declarative way to compose chains, offering features like streaming, batching, and async support. The `|` (pipe) operator is central to LCEL, allowing you to chain components together.\n",
    "* **Components:** LangChain provides modular components for building applications, including:\n",
    "    * **Models:** Interfaces to various language models (e.g., OpenAI, Anthropic, Google).\n",
    "    * **Prompts:** Templates for generating prompts for LLMs.\n",
    "    * **Output Parsers:** Tools to structure the output from LLMs.\n",
    "* **Retrieval-Augmented Generation (RAG):** A powerful technique to connect LLMs to external data sources, enhancing their knowledge and providing more accurate, up-to-date responses.\n",
    "\n",
    "**Installation**\n",
    "\n",
    "First, let's install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core LangChain and provider-specific packages\n",
    "!pip install langchain langchain-core langchain-community langchain-openai langchain-chroma faiss-cpu pypdf sentence-transformers tiktoken -q\n",
    "\n",
    "# Install environment management and web request libraries\n",
    "!pip install python-dotenv requests beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Setup**\n",
    "\n",
    "Configure your API keys. It's recommended to use environment variables for security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"‚úÖ API keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Building Your First Chain with LCEL**\n",
    "\n",
    "A \"chain\" in LangChain is a sequence of calls to components. We'll use the LangChain Expression Language (LCEL) to build a simple chain.\n",
    "\n",
    "**Initialize the Model**\n",
    "\n",
    "We'll start by initializing a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized model: ChatOpenAI\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize a chat model from OpenAI\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
    "\n",
    "print(f\"‚úÖ Initialized model: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work with Prompt Templates**\n",
    "\n",
    "Prompt templates allow you to create reusable and parameterized prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in {expertise}.\"),\n",
    "    (\"human\", \"Explain {topic} in a simple and concise way.\")\n",
    "])\n",
    "\n",
    "# Create a simple chain using LCEL\n",
    "# The chain will:\n",
    "# 1. Take user input for 'expertise' and 'topic'.\n",
    "# 2. Format the prompt using the prompt_template.\n",
    "# 3. Pass the formatted prompt to the model.\n",
    "# 4. Parse the model's output to a string.\n",
    "simple_chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "response = simple_chain.invoke({\n",
    "    \"expertise\": \"physics\",\n",
    "    \"topic\": \"quantum entanglement\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming Responses**\n",
    "\n",
    "For a better user experience, you can stream the model's response as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåä Streaming response:\")\n",
    "for chunk in simple_chain.stream({\n",
    "    \"expertise\": \"culinary arts\",\n",
    "    \"topic\": \"the Maillard reaction\"\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Structured Output and Advanced Chains**\n",
    "\n",
    "LangChain can parse model outputs into structured formats and build more complex, conditional chains.\n",
    "\n",
    "**Pydantic Output Parser**\n",
    "\n",
    "Use Pydantic models to define the desired output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define a Pydantic model for structured output\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"The name of the recipe\")\n",
    "    ingredients: List[str] = Field(description=\"A list of ingredients\")\n",
    "    steps: List[str] = Field(description=\"The steps to prepare the recipe\")\n",
    "\n",
    "# Create a Pydantic output parser\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# Create a prompt that includes format instructions\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class chef. Generate a recipe based on the user's request and format it as requested.\"),\n",
    "    (\"human\", \"I want a simple recipe for {dish}.\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Create the structured output chain\n",
    "structured_chain = structured_prompt | model | pydantic_parser\n",
    "\n",
    "# Invoke the chain\n",
    "recipe_request = {\n",
    "    \"dish\": \"scrambled eggs\",\n",
    "    \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "}\n",
    "recipe_output = structured_chain.invoke(recipe_request)\n",
    "\n",
    "print(f\"Recipe for: {recipe_output.name}\")\n",
    "print(\"\\nIngredients:\")\n",
    "for ingredient in recipe_output.ingredients:\n",
    "    print(f\"- {ingredient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Chains with `RunnableBranch`**\n",
    "\n",
    "Create dynamic chains that change their behavior based on input conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Define different prompts for different levels\n",
    "beginner_prompt = ChatPromptTemplate.from_template(\"Explain {topic} to a complete beginner.\")\n",
    "expert_prompt = ChatPromptTemplate.from_template(\"Provide a detailed, technical explanation of {topic}.\")\n",
    "\n",
    "# Create a conditional chain using RunnableBranch\n",
    "# This chain checks the 'level' input and routes to the appropriate prompt\n",
    "conditional_chain = (\n",
    "    RunnableBranch(\n",
    "        (lambda x: x.get(\"level\") == \"expert\", expert_prompt),\n",
    "        beginner_prompt  # Default prompt\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the beginner path\n",
    "beginner_response = conditional_chain.invoke({\"topic\": \"black holes\", \"level\": \"beginner\"})\n",
    "print(\"--- Beginner Explanation ---\")\n",
    "print(beginner_response)\n",
    "\n",
    "# Test the expert path\n",
    "expert_response = conditional_chain.invoke({\"topic\": \"black holes\", \"level\": \"expert\"})\n",
    "print(\"\\n--- Expert Explanation ---\")\n",
    "print(expert_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "RAG connects your LLM to external data, allowing it to answer questions about information it wasn't trained on.\n",
    "\n",
    "**Step 1: Document Loading and Splitting**\n",
    "\n",
    "Load data from a source (like a website) and split it into smaller chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents from a web page\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/modules/model_io/prompts/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Split the documents into chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents and split them into {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Embeddings and Vector Stores**\n",
    "\n",
    "Convert the text chunks into numerical representations (embeddings) and store them in a vector database for efficient searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a Chroma vector store from the document splits\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "print(\"‚úÖ Vector store created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Creating a Retriever**\n",
    "\n",
    "A retriever is responsible for finding the most relevant document chunks based on a user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test the retriever\n",
    "query = \"What are prompt templates?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for the query: '{query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Building the RAG Chain**\n",
    "\n",
    "Now, let's combine the retriever with a prompt and the LLM to create a complete RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the following context to answer the user's question.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Helper function to format the retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the RAG chain\n",
    "rag_question = \"How can I use few-shot examples in my prompts?\"\n",
    "rag_answer = rag_chain.invoke(rag_question)\n",
    "\n",
    "print(f\"\\n‚ùì Question: {rag_question}\")\n",
    "print(f\"üéØ Answer: {rag_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
