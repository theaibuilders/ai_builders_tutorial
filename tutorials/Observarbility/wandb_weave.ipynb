{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Weave: A Hands-On Tutorial\n",
    "\n",
    "Welcome to this tutorial on W&B Weave! üöÄ\n",
    "\n",
    "**W&B Weave** is a powerful toolkit for developers building applications with Large Language Models (LLMs). It helps you track, visualize, debug, and evaluate your LLM-powered applications, making the development process more rigorous and efficient. Whether you're building a simple chatbot or a complex Retrieval-Augmented Generation (RAG) system, Weave provides the tools you need to understand and improve your application's performance.\n",
    "\n",
    "In this notebook, we'll cover the most important concepts of Weave and walk you through a hands-on example of building and evaluating a simple RAG system. By the end of this tutorial, you'll be able to:\n",
    "\n",
    "- **Trace your Python functions** with a single decorator to understand their execution flow.\n",
    "- **Automatically capture LLM calls** to services like OpenAI.\n",
    "- **Build and debug a RAG system** with full visibility into each step.\n",
    "- **Evaluate your application's performance** using custom scorers.\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the necessary libraries. We'll need `weave` for tracing and evaluation, `wandb` for logging, `openai` to interact with the GPT models, `python-dotenv` to manage our API keys, and `scikit-learn` for our RAG example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install weave wandb openai sklearn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Environment Variables\n",
    "\n",
    "It's a best practice to store your API keys and other sensitive information in a `.env` file. This keeps your secrets out of your code and makes it easy to manage different environments.\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook and add your API keys like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY=\"your_wandb_api_key\"\n",
    "OPENAI_API_KEY=\"your_openai_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can get your W&B API key from the [W&B Authorize page](https://wandb.ai/authorize) and your OpenAI API key from the [OpenAI API keys page](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Now you can access your API keys using os.getenv()\n",
    "os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not os.environ[\"WANDB_API_KEY\"] or not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    print(\"API keys not found. Make sure to create a .env file with your WANDB_API_KEY and OPENAI_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Initialize Weave\n",
    "\n",
    "Now, let's initialize Weave. This will set up a new project in your W&B account where all your traces and evaluations will be logged. If the project doesn't exist, it will be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: devonsun_ml.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/datumverse/ai-builders-tutorial/weave\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "weave_client = weave.init(\"ai-builders-tutorial\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Concepts: Tracing with `@weave.op()`\n",
    "\n",
    "The core of Weave's debugging capabilities is **tracing**. A trace is a record of the execution of a function, including its inputs, outputs, and any sub-operations it calls. The easiest way to create a trace is with the `@weave.op()` decorator.\n",
    "\n",
    "Let's create a simple function and decorate it with `@weave.op()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/datumverse/ai-builders-tutorial/r/call/0198ea3b-e1cc-7667-acfe-4a988a0d00bb\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/datumverse/ai-builders-tutorial/r/call/0198ea3d-9099-7c43-a182-b3cd18aeda90\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/datumverse/ai-builders-tutorial/r/call/0198ea3f-0efb-7d54-9e7d-58891b710682\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def add(a, b):\n",
    "    \"\"\"A simple function to add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Call the decorated function\n",
    "result = add(5, 10)\n",
    "print(f\"The result is: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell above, Weave will log a trace of the `add` function's execution. You can view this trace in your W&B project. The trace will show the inputs (`a=5`, `b=10`) and the output (`15`). This is a simple example, but it demonstrates how powerful tracing can be for understanding what your code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic LLM Tracing\n",
    "\n",
    "Weave automatically traces calls to many popular LLM libraries, including OpenAI. This means you don't need to add any special decorators to your LLM calls to get full visibility into the prompts, responses, token usage, and more.\n",
    "\n",
    "Let's make a simple call to the OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/datumverse/ai-builders-tutorial/r/call/0198ea3c-bf9d-7926-a828-aa26c8e6b131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of France?\n",
      "Response: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op()\n",
    "def generate_text(prompt):\n",
    "    \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = generate_text(prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your W&B project, you'll see a trace for the `generate_text` function. If you expand the trace, you'll see the underlying OpenAI API call with all the details, including the model used, the prompt, the completion, and even the token counts. This is incredibly useful for debugging your interactions with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On Example: Building and Tracing a RAG System\n",
    "\n",
    "Now, let's put it all together and build a simple Retrieval-Augmented Generation (RAG) system. A RAG system first retrieves relevant documents from a knowledge base and then uses those documents as context for an LLM to generate an answer.\n",
    "\n",
    "We'll trace each step of our RAG system with `@weave.op()` to get a clear picture of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Where is the Eiffel Tower?\n",
      "Answer: The Eiffel Tower is located in Paris, France.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Our knowledge base (a simple list of documents)\n",
    "documents = [\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"The Great Wall of China is one of the seven wonders of the world.\",\n",
    "    \"The Statue of Liberty is in New York City, USA.\",\n",
    "    \"The Colosseum is an ancient amphitheater in Rome, Italy.\"\n",
    "]\n",
    "\n",
    "# 2. Create a simple retriever\n",
    "vectorizer = TfidfVectorizer().fit(documents)\n",
    "doc_vectors = vectorizer.transform(documents)\n",
    "\n",
    "@weave.op()\n",
    "def retrieve_documents(query, k=1):\n",
    "    \"\"\"Retrieves the top k most relevant documents for a given query.\"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    return [documents[i] for i in top_k_indices]\n",
    "\n",
    "# 3. Create the RAG chain\n",
    "@weave.op()\n",
    "def rag_chain(query):\n",
    "    \"\"\"Our RAG system: retrieves documents and then generates an answer.\"\"\"\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    \n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"Based on the following context, answer the user's query.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"\n",
    "    \n",
    "    return generate_text(prompt)\n",
    "\n",
    "# 4. Run the RAG system\n",
    "query = \"Where is the Eiffel Tower?\"\n",
    "answer = rag_chain(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you view the trace for this `rag_chain` call in W&B, you'll see a nested structure. You can see the top-level `rag_chain` call, and inside it, you can see the `retrieve_documents` call and the `generate_text` call. This allows you to inspect the inputs and outputs of each component of your system, which is invaluable for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation with Weave\n",
    "\n",
    "Now that we have a working RAG system, how do we know if it's any good? This is where **evaluation** comes in. Weave provides a framework for evaluating your LLM applications.\n",
    "\n",
    "An **evaluation** consists of:\n",
    "\n",
    "- A **model** (in our case, the `rag_chain` function).\n",
    "- A **dataset** of examples to test the model on.\n",
    "- A set of **scorers** that measure the quality of the model's outputs.\n",
    "\n",
    "Let's create a simple evaluation for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_attempt\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"ContainsScorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"contains_expected\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 2,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 0.7733820676803589\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"ContainsScorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"contains_expected\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 2,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 0.7733820676803589\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0198ea45-2914-7e5f-a9ed-7debd467703d': {}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "dataset = [\n",
    "    {\"query\": \"Where is the Eiffel Tower?\", \"expected_answer\": \"Paris\"},\n",
    "    {\"query\": \"What is in Rome?\", \"expected_answer\": \"Colosseum\"},\n",
    "]\n",
    "\n",
    "class ContainsScorer(weave.Scorer):\n",
    "    @weave.op()\n",
    "    def score(self, output: str, expected_answer: str) -> dict:\n",
    "        return {\"contains_expected\": expected_answer.lower() in (output or \"\").lower()}\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=dataset,\n",
    "    scorers=[ContainsScorer()],\n",
    "    evaluation_name=\"rag_contains_eval\",\n",
    ")\n",
    "\n",
    "results = await evaluation.evaluate(rag_chain)\n",
    "evaluation.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this evaluation, Weave will call your `rag_chain` function for each example in the dataset and then run your `ContainsScorer` on the output. The results will be displayed in a beautiful dashboard in your W&B project, where you can see the scores for each example and get an aggregate view of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WandB Weave](/public/products/wandb_weave.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Congratulations! üéâ You've successfully built, traced, and evaluated an LLM-powered application with W&B Weave.\n",
    "\n",
    "In this tutorial, we've covered the fundamental concepts of Weave:\n",
    "\n",
    "- **Tracing** with `@weave.op()` to get visibility into your code's execution.\n",
    "- **Automatic logging** of LLM calls.\n",
    "- **Building and debugging** a RAG system.\n",
    "- **Evaluating** your application's performance with custom scorers.\n",
    "\n",
    "This is just the beginning of what you can do with Weave. To learn more, check out the official W&B Weave documentation. \n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
