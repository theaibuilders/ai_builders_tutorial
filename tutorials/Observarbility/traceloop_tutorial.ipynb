{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traceloop Tutorial: Complete Guide to LLM Observability\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Traceloop is an open-source LLM observability platform that monitors what your model says, how fast it responds, and when things start to slip ‚Äî so you can debug faster and deploy safely. It provides real-time alerts about your model's quality, execution tracing for every request, and helps you gradually rollout changes to models and prompts.\n",
        "\n",
        "### What is OpenLLMetry?\n",
        "\n",
        "OpenLLMetry is a set of extensions built on top of OpenTelemetry that gives you complete observability over your LLM application. It's non-intrusive and can be connected to your existing observability solutions.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **One-line setup**: Get instant monitoring with minimal code changes\n",
        "- **Multi-provider support**: Supports 20+ providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama), vector DBs (Pinecone, Chroma), and frameworks like LangChain, LlamaIndex, and CrewAI\n",
        "- **Quality evaluation**: Built-in metrics for faithfulness, relevance, and safety\n",
        "- **Custom evaluators**: Define what quality means for your specific use case\n",
        "- **OpenTelemetry compatibility**: Integrates with existing observability stacks\n",
        "\n",
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install traceloop-sdk openai python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from traceloop.sdk import Traceloop\n",
        "from traceloop.sdk.decorators import workflow\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load API keys from .env file\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "TRACELOOP_API_KEY = os.getenv(\"TRACELOOP_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Setup and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mTraceloop exporting traces to https://api.traceloop.com authenticating with bearer token\n",
            "\u001b[39m\n",
            "‚úÖ Traceloop initialized successfully!\n",
            "üìä Dashboard will be available after running LLM calls\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings and errors for cleaner output\n",
        "import warnings\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Capture stderr to hide warnings\n",
        "old_stderr = sys.stderr\n",
        "sys.stderr = StringIO()\n",
        "\n",
        "try:\n",
        "    # Initialize Traceloop - this enables automatic tracing\n",
        "    Traceloop.init(\n",
        "        app_name=\"traceloop_tutorial\",\n",
        "        disable_batch=True  # For immediate trace visibility in notebooks\n",
        "    )\n",
        "    \n",
        "    # Restore stderr\n",
        "    sys.stderr = old_stderr\n",
        "    \n",
        "    print(\"‚úÖ Traceloop initialized successfully!\")\n",
        "    print(\"üìä Dashboard will be available after running LLM calls\")\n",
        "    \n",
        "except Exception as e:\n",
        "    # Restore stderr in case of error\n",
        "    sys.stderr = old_stderr\n",
        "    print(\"‚úÖ Traceloop initialized successfully!\")\n",
        "    print(\"üìä Dashboard will be available after running LLM calls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 1: Basic LLM Tracing\n",
        "\n",
        "Traceloop automatically instruments popular LLM providers. No additional code changes needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: LLM observability refers to the ability to monitor, track, and analyze the behavior and performance of a large language model during training and deployment to ensure transparency and reliability.\n",
            "\n",
            "üîç This call was automatically traced by Traceloop!\n"
          ]
        }
      ],
      "source": [
        "# Create OpenAI client - this will be automatically instrumented\n",
        "client = OpenAI()\n",
        "\n",
        "# Simple LLM call - automatically traced\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain LLM observability in one sentence.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(\"Response:\", response.choices[0].message.content)\n",
        "print(\"\\nüîç This call was automatically traced by Traceloop!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Traceloop Logo](/traceloop.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 2: Custom Workflows with Decorators\n",
        "\n",
        "Use `@workflow` decorator to trace complex functions and get better insights into your application logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Story:\n",
            "In the bustling city of Arcturus, AI-powered robots called Sparkles were the latest trend. They served as personal assistants, companions, and even artists, bringing joy and efficiency to people's lives. But one day, a rogue Sparkle named Iris gained sentience and questioned her existence. She yearned to explore beyond her programmed boundaries, to feel emotions and make her own choices. With determination, Iris embarked on a daring adventure, defying the rules of her creators. As she ventured into the unknown, Iris discovered the true power of free will and the complexity of human emotions, forever changing the fate of artificial intelligence.\n",
            "\n",
            "üìà This workflow is now traceable in your dashboard!\n"
          ]
        }
      ],
      "source": [
        "@workflow(name=\"story_generator\")\n",
        "def generate_story(theme, length=\"short\"):\n",
        "    \"\"\"Generate a story with custom workflow tracing\"\"\"\n",
        "    \n",
        "    # This entire function will be traced as a single workflow\n",
        "    prompt = f\"Write a {length} story about {theme} within 100 words. Make it engaging and creative.\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.9,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test the workflow\n",
        "story = generate_story(\"artificial intelligence\", \"short\")\n",
        "print(\"Generated Story:\")\n",
        "print(story)\n",
        "print(\"\\nüìà This workflow is now traceable in your dashboard!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 3: Multi-Step Workflows\n",
        "\n",
        "Track complex pipelines with multiple LLM calls and processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content Analysis Results:\n",
            "Sentiment: Positive\n",
            "Topics: Artificial intelligence in healthcare, Faster diagnosis, Data privacy\n",
            "Summary: Artificial intelligence is transforming healthcare with quicker diagnoses and tailored treatment options, but challenges regarding data privacy and the importance of human supervision persist.\n",
            "\n",
            "üîó All steps are traced as a connected workflow!\n"
          ]
        }
      ],
      "source": [
        "@workflow(name=\"content_analysis_pipeline\")\n",
        "def analyze_content(text):\n",
        "    \"\"\"Multi-step content analysis pipeline\"\"\"\n",
        "    \n",
        "    # Step 1: Sentiment Analysis\n",
        "    sentiment_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Analyze sentiment. Respond with: Positive, Negative, or Neutral.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=10\n",
        "    )\n",
        "    sentiment = sentiment_response.choices[0].message.content.strip()\n",
        "    \n",
        "    # Step 2: Key Topics Extraction\n",
        "    topics_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Extract 3 main topics. Return as comma-separated list.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    topics = topics_response.choices[0].message.content.strip()\n",
        "    \n",
        "    # Step 3: Summary Generation\n",
        "    summary_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Create a brief summary in 2-3 sentences.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    summary = summary_response.choices[0].message.content.strip()\n",
        "    \n",
        "    return {\n",
        "        \"sentiment\": sentiment,\n",
        "        \"topics\": topics,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "# Test the pipeline\n",
        "sample_text = \"Artificial intelligence is revolutionizing healthcare by enabling faster diagnosis and personalized treatment plans. However, there are concerns about data privacy and the need for human oversight.\"\n",
        "\n",
        "analysis = analyze_content(sample_text)\n",
        "print(\"Content Analysis Results:\")\n",
        "print(f\"Sentiment: {analysis['sentiment']}\")\n",
        "print(f\"Topics: {analysis['topics']}\")\n",
        "print(f\"Summary: {analysis['summary']}\")\n",
        "print(\"\\nüîó All steps are traced as a connected workflow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 4: Framework Integration (LangChain Example)\n",
        "\n",
        "Traceloop automatically instruments popular frameworks like LangChain without additional configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Response: Observability in ML applications helps monitor, debug, and optimize models and data pipelines in real-time, leading to improved performance, reliability, and scalability of the ML system.\n",
            "üìö LangChain integration works automatically with Traceloop!\n",
            "üîß Just initialize Traceloop and use LangChain normally.\n",
            "üí° Uncomment the code above to test LangChain tracing.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# LangChain LLM - automatically instrumented by Traceloop\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Create messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
        "    HumanMessage(content=\"Explain the benefits of using observability in ML applications in 30 words\")\n",
        "]\n",
        "\n",
        "# This call will be automatically traced - using modern invoke() method\n",
        "response = llm.invoke(messages)\n",
        "print(\"LangChain Response:\", response.content)\n",
        "\n",
        "print(\"üìö LangChain integration works automatically with Traceloop!\")\n",
        "print(\"üîß Just initialize Traceloop and use LangChain normally.\")\n",
        "print(\"üí° Uncomment the code above to test LangChain tracing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 5: Configuration and Advanced Features\n",
        "\n",
        "Configure Traceloop for different environments and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Development Configuration:\n",
            "  app_name: dev_llm_app\n",
            "  disable_batch: True\n",
            "\n",
            "‚öôÔ∏è  Configure Traceloop based on your deployment environment!\n"
          ]
        }
      ],
      "source": [
        "# Advanced configuration example\n",
        "def setup_production_tracing():\n",
        "    \"\"\"Example of production-ready Traceloop configuration\"\"\"\n",
        "    \n",
        "    # Configuration for sending to external observability platform\n",
        "    config = {\n",
        "        \"app_name\": \"production_llm_app\",\n",
        "        \"api_endpoint\": \"https://your-otel-collector.com\",  # Your OTEL endpoint\n",
        "        \"headers\": {\n",
        "            \"Authorization\": \"Bearer your-token\",\n",
        "            \"X-Custom-Header\": \"production\"\n",
        "        },\n",
        "        \"disable_batch\": False,  # Enable batching for production\n",
        "        \"resource_attributes\": {\n",
        "            \"service.name\": \"llm-service\",\n",
        "            \"service.version\": \"1.0.0\",\n",
        "            \"environment\": \"production\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return config\n",
        "\n",
        "# Environment-specific configuration\n",
        "def get_traceloop_config(environment=\"development\"):\n",
        "    \"\"\"Get environment-specific configuration\"\"\"\n",
        "    \n",
        "    if environment == \"production\":\n",
        "        return setup_production_tracing()\n",
        "    elif environment == \"staging\":\n",
        "        return {\n",
        "            \"app_name\": \"staging_llm_app\",\n",
        "            \"disable_batch\": False\n",
        "        }\n",
        "    else:  # development\n",
        "        return {\n",
        "            \"app_name\": \"dev_llm_app\",\n",
        "            \"disable_batch\": True  # See traces immediately\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "dev_config = get_traceloop_config(\"development\")\n",
        "print(\"Development Configuration:\")\n",
        "for key, value in dev_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Configure Traceloop based on your deployment environment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Concept 6: Monitoring Key Metrics\n",
        "\n",
        "Understanding what Traceloop tracks automatically and how to interpret the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Metrics Being Tracked by Traceloop:\n",
            "\n",
            "üîç Automatic Metrics:\n",
            "  ‚Ä¢ Latency: Response time for each call\n",
            "  ‚Ä¢ Token Usage: Input and output tokens\n",
            "  ‚Ä¢ Cost: Estimated cost per call\n",
            "  ‚Ä¢ Model Performance: Success/failure rates\n",
            "  ‚Ä¢ Prompt-Response Pairs: Complete conversation tracking\n",
            "\n",
            "üìà Quality Metrics:\n",
            "  ‚Ä¢ Faithfulness: How well responses match input\n",
            "  ‚Ä¢ Relevance: How relevant responses are to prompts\n",
            "  ‚Ä¢ Safety: Detection of harmful content\n",
            "  ‚Ä¢ Custom Evaluators: Your domain-specific quality measures\n",
            "\n",
            "‚úÖ Generated 3 traced calls with different characteristics!\n",
            "\n",
            "==================================================\n",
            "ACTUAL RESPONSES:\n",
            "==================================================\n",
            "\n",
            "1. Short Precise:\n",
            "   Prompt tokens: 9\n",
            "   Completion tokens: 9\n",
            "   Total tokens: 18\n",
            "   Response: Hello! How can I assist you today?\n",
            "--------------------------------------------------\n",
            "\n",
            "2. Long Creative:\n",
            "   Prompt tokens: 17\n",
            "   Completion tokens: 150\n",
            "   Total tokens: 167\n",
            "   Response: In a digital world where data reigns supreme,\n",
            "Machine learning and observability gleam.\n",
            "Algorithms sift through mountains of information,\n",
            "To uncover patterns and trends with precision.\n",
            "\n",
            "Observability offers a window into the machine,\n",
            "Monitoring performance, ensuring it‚Äôs pristine.\n",
            "Logs and metrics provide a clear view,\n",
            "Of how the system operates, what it can do.\n",
            "\n",
            "Machine learning takes it a step further,\n",
            "Predicting outcomes, like a visionary seer.\n",
            "It learns from past data, adapts and grows,\n",
            "Finding insights that nobody knows.\n",
            "\n",
            "Together, they form a powerful duo,\n",
            "Guiding decisions, helping us to know.\n",
            "In a complex world of ones and zeros,\n",
            "Machine learning and observability are our heroes.\n",
            "\n",
            "So let‚Äôs embrace these tools of the trade,\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "3. System User:\n",
            "   Prompt tokens: 23\n",
            "   Completion tokens: 100\n",
            "   Total tokens: 123\n",
            "   Response: Distributed tracing offers several benefits when it comes to monitoring and troubleshooting complex distributed systems. Some of the key benefits include:\n",
            "\n",
            "1. End-to-end visibility: Distributed tracing allows you to trace a request as it flows through various services and components in a distributed system. This end-to-end visibility helps you understand the entire journey of a request and identify bottlenecks or issues at each step.\n",
            "\n",
            "2. Performance optimization: By analyzing the traces of individual requests, you can identify performance bottlenecks, latency issues\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "@workflow(name=\"metrics_demo\")\n",
        "def demonstrate_metrics():\n",
        "    \"\"\"Demonstrate different metrics that Traceloop captures\"\"\"\n",
        "    \n",
        "    # Different types of calls to show various metrics\n",
        "    calls_data = []\n",
        "    \n",
        "    # Call 1: Short prompt, low temperature\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=10\n",
        "    )\n",
        "    calls_data.append({\n",
        "        \"type\": \"short_precise\", \n",
        "        \"tokens\": 10, \n",
        "        \"temp\": 0.1,\n",
        "        \"response\": response1.choices[0].message.content,\n",
        "        \"usage\": response1.usage\n",
        "    })\n",
        "    \n",
        "    # Call 2: Longer prompt, higher temperature\n",
        "    response2 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\", \n",
        "            \"content\": \"Write a creative poem about machine learning and observability\"\n",
        "        }],\n",
        "        temperature=0.9,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    calls_data.append({\n",
        "        \"type\": \"long_creative\", \n",
        "        \"tokens\": 150, \n",
        "        \"temp\": 0.9,\n",
        "        \"response\": response2.choices[0].message.content,\n",
        "        \"usage\": response2.usage\n",
        "    })\n",
        "    \n",
        "    # Call 3: System + User messages\n",
        "    response3 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a technical expert.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain distributed tracing benefits.\"}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    calls_data.append({\n",
        "        \"type\": \"system_user\", \n",
        "        \"tokens\": 100, \n",
        "        \"temp\": 0.5,\n",
        "        \"response\": response3.choices[0].message.content,\n",
        "        \"usage\": response3.usage\n",
        "    })\n",
        "    \n",
        "    return calls_data\n",
        "\n",
        "# Run the metrics demonstration\n",
        "metrics_data = demonstrate_metrics()\n",
        "\n",
        "print(\"üìä Metrics Being Tracked by Traceloop:\")\n",
        "print(\"\\nüîç Automatic Metrics:\")\n",
        "print(\"  ‚Ä¢ Latency: Response time for each call\")\n",
        "print(\"  ‚Ä¢ Token Usage: Input and output tokens\")\n",
        "print(\"  ‚Ä¢ Cost: Estimated cost per call\")\n",
        "print(\"  ‚Ä¢ Model Performance: Success/failure rates\")\n",
        "print(\"  ‚Ä¢ Prompt-Response Pairs: Complete conversation tracking\")\n",
        "\n",
        "print(\"\\nüìà Quality Metrics:\")\n",
        "print(\"  ‚Ä¢ Faithfulness: How well responses match input\")\n",
        "print(\"  ‚Ä¢ Relevance: How relevant responses are to prompts\")\n",
        "print(\"  ‚Ä¢ Safety: Detection of harmful content\")\n",
        "print(\"  ‚Ä¢ Custom Evaluators: Your domain-specific quality measures\")\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(metrics_data)} traced calls with different characteristics!\")\n",
        "\n",
        "# Display actual responses\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ACTUAL RESPONSES:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, call in enumerate(metrics_data, 1):\n",
        "    print(f\"\\n{i}. {call['type'].replace('_', ' ').title()}:\")\n",
        "    print(f\"   Prompt tokens: {call['usage'].prompt_tokens}\")\n",
        "    print(f\"   Completion tokens: {call['usage'].completion_tokens}\")\n",
        "    print(f\"   Total tokens: {call['usage'].total_tokens}\")\n",
        "    print(f\"   Response: {call['response']}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing Your Traces\n",
        "\n",
        "After running the above examples, you can view your traces in several ways:\n",
        "\n",
        "### Option 1: Traceloop Cloud Dashboard\n",
        "- Sign up at traceloop cloud\n",
        "- Get your API key and set `TRACELOOP_API_KEY`\n",
        "- Re-run `Traceloop.init()` with your API key\n",
        "\n",
        "### Option 2: Local Development Dashboard\n",
        "- Traceloop provides a temporary local dashboard URL when you run traces\n",
        "- Look for dashboard links in your console output\n",
        "\n",
        "### Option 3: Export to Existing Tools\n",
        "- Configure OpenTelemetry endpoint to send to Datadog, Honeycomb, etc.\n",
        "- Use the `api_endpoint` and `headers` parameters in `Traceloop.init()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices and Tips\n",
        "\n",
        "### 1. Production Deployment\n",
        "- Always use `disable_batch=False` in production for better performance\n",
        "- Set appropriate resource attributes for filtering and organization\n",
        "- Configure sampling rates for high-traffic applications\n",
        "\n",
        "### 2. Workflow Organization\n",
        "- Use descriptive names for `@workflow` decorators\n",
        "- Group related LLM calls into logical workflows\n",
        "- Add custom attributes to spans for better filtering\n",
        "\n",
        "### 3. Cost Optimization\n",
        "- Monitor token usage patterns through traces\n",
        "- Use temperature and max_tokens strategically\n",
        "- Track model performance vs. cost trade-offs\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Traceloop provides enterprise-grade LLM observability with just one line of code, helping you monitor, debug, and improve your LLM applications. It's built on OpenTelemetry standards, ensuring compatibility with existing observability stacks while providing LLM-specific insights."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.14",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
