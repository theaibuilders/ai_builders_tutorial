{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ad4166",
   "metadata": {},
   "source": [
    "# Together AI: Fine-Tuning LLMs\n",
    "\n",
    "This notebook provides a step-by-step guide to fine-tuning Large Language Models (LLMs) using the Together AI platform. We will cover the entire workflow, from preparing your dataset to making inference calls with your new custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ecd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "git add -A && git commit -m \"Update project ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))\" && git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b4f93",
   "metadata": {},
   "source": [
    "## üí° Key Concepts in Fine-Tuning\n",
    "\n",
    "Before we write code, let's grasp the core ideas:\n",
    "\n",
    "- **Fine-Tuning**: This is the process of taking a general-purpose, pre-trained LLM and further training it on a smaller, specific dataset. This adapts the model to your particular domain or task, such as a customer support chatbot or a code generator for a specific programming language.\n",
    "\n",
    "- **Dataset Formatting**: The quality and format of your data are critical. For instruction-based fine-tuning, you need to structure your data with clear prompts and desired responses. Together AI expects data in a **JSONL** format, where each line is a JSON object containing a `\"text\"` field.\n",
    "\n",
    "- **Base Model**: This is the pre-trained model you start with. Your choice of base model is important. For example, a model pre-trained on chat is a better starting point for a chatbot than a raw text completion model. Together AI offers many state-of-the-art open-source models.\n",
    "\n",
    "- **Hyperparameters**: These are the settings for your training job, such as `learning_rate`, `batch_size`, and the number of `epochs` (how many times the model sees the entire dataset). Tuning these can significantly impact your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d4001",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup and Installation\n",
    "\n",
    "First, we need to install the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d805bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the required packages\n",
    "# %pip install -U together datasets transformers python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b421ac2",
   "metadata": {},
   "source": [
    "### Loading API Keys\n",
    "We'll use the `dotenv` library to securely load our Together AI API key from a `.env` file. Create a file named `.env` in the same directory as this notebook and add your key:\n",
    "\n",
    "```\n",
    "TOGETHER_API_KEY=\"your-together-api-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8a4d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import together\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys from environment variables\n",
    "os.environ[\"TOGETHER_API_KEY\"] = os.environ.get(\"TOGETHER_API_KEY\", \"\")\n",
    "os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\", \"\")\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "HUGGINGFACE_ACCESS_TOKEN = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30d3b3",
   "metadata": {},
   "source": [
    "## üìÇ 2. Data Preparation\n",
    "\n",
    "We will use a small sample from the `databricks/databricks-dolly-15k` dataset. We'll format it into the required JSONL structure using a standard instruction template (`<s>[INST]...[/INST]...</s>`) that works well with models like Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "001f5326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved to dolly_prepared.jsonl\n",
      "--- Sample Entry ---\n",
      "<s>[INST] When did Virgin Australia start operating? [/INST] Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. </s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample of 500 examples from the dataset\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\", token=HUGGINGFACE_ACCESS_TOKEN).select(range(500))\n",
    "\n",
    "def format_for_finetuning(example):\n",
    "    # Use a standard instruction format\n",
    "    return {\"text\": f\"<s>[INST] {example['instruction']} [/INST] {example['response']} </s>\"}\n",
    "\n",
    "formatted_dataset = dataset.map(format_for_finetuning)\n",
    "\n",
    "# Save the prepared data to a JSONL file (only the 'text' field per line)\n",
    "file_name = \"dolly_prepared.jsonl\"\n",
    "with open(file_name, 'w') as f:\n",
    "    for item in formatted_dataset:\n",
    "        f.write(json.dumps({\"text\": item[\"text\"]}) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset prepared and saved to {file_name}\")\n",
    "print(\"--- Sample Entry ---\")\n",
    "with open(file_name, 'r') as f:\n",
    "    print(json.loads(f.readline())['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856e247",
   "metadata": {},
   "source": [
    "## üöÄ 3. Upload File & Start Fine-Tuning\n",
    "\n",
    "Now we upload our prepared dataset and launch the fine-tuning job. We will fine-tune the `togethercomputer/llama-2-7b-chat` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "692f2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/g_b0j0n53rz5fm8yrlw3jg040000gn/T/ipykernel_55499/1191766534.py:3: DeprecationWarning: Call to deprecated function upload.\n",
      "  upload_response = together.Files.upload(file=file_name)\n",
      "Uploading file dolly_prepared.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 266k/266k [00:01<00:00, 212kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully. File ID: file-9984a196-2c4b-4f82-b44e-da96665f34b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/g_b0j0n53rz5fm8yrlw3jg040000gn/T/ipykernel_55499/1191766534.py:8: DeprecationWarning: Call to deprecated function create.\n",
      "  fine_tune_response = together.Finetune.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning job created:\n",
      "{'id': 'ft-91b93aa1-b4dd', 'training_file': 'file-9984a196-2c4b-4f82-b44e-da96665f34b1', 'model': 'togethercomputer/llama-2-7b-chat', 'n_epochs': 3, 'n_checkpoints': 1, 'n_evals': 0, 'batch_size': 8, 'learning_rate': 1e-05, 'lr_scheduler': {'lr_scheduler_type': 'cosine', 'lr_scheduler_args': {'min_lr_ratio': 0.0, 'num_cycles': 0.5}}, 'warmup_ratio': 0.0, 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'eval_steps': 0, 'training_type': {'type': 'Lora'}, 'created_at': '2025-08-05T17:54:21.187Z', 'updated_at': '2025-08-05T17:54:21.187Z', 'status': <FinetuneJobStatus.STATUS_PENDING: 'pending'>, 'events': [], 'token_count': 0, 'total_price': 0, 'wandb_base_url': '', 'wandb_project_name': '', 'wandb_name': '', 'train_on_inputs': 'auto', 'suffix': 'dolly-llama2-7b-tutorial', 'training_method': {'method': 'sft', 'train_on_inputs': 'auto'}, 'random_seed': 'null', 'max_steps': -1, 'save_steps': 0, 'warmup_steps': 0, 'validation_split_ratio': 0, 'per_device_batch_size': 0, 'per_device_eval_batch_size': 0, 'gradient_accumulation_steps': 1, 'continued_checkpoint': '', 'merge_parent_adapter': False, 'parent_ft_id': '', 'try_byoa_upload': True, 'user_id': '68920bddaeed77e341146664', 'owner_address': '0xb8e99171f6536df47bc53657526b6dbdcfbc0ee9'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Upload the training file\n",
    "try:\n",
    "    upload_response = together.Files.upload(file=file_name)\n",
    "    training_file_id = upload_response['id']\n",
    "    print(f\"File uploaded successfully. File ID: {training_file_id}\")\n",
    "\n",
    "    # 2. Create the fine-tuning job\n",
    "    fine_tune_response = together.Finetune.create(\n",
    "      training_file=training_file_id,\n",
    "      model='togethercomputer/llama-2-7b-chat', # Base model to fine-tune\n",
    "      n_epochs=3,                            # Number of training epochs\n",
    "      n_checkpoints=1,                       # Number of checkpoints to save\n",
    "      batch_size=8,                          # Batch size\n",
    "      learning_rate=1e-5,                    # Learning rate\n",
    "      suffix='dolly-llama2-7b-tutorial',     # A custom name for your fine-tuned model\n",
    "    )\n",
    "\n",
    "    print(\"\\nFine-tuning job created:\")\n",
    "    print(fine_tune_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file or creating fine-tune job: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29cbb0",
   "metadata": {},
   "source": [
    "## üîé 4. Monitor the Fine-Tuning Job\n",
    "\n",
    "The fine-tuning process can take some time. You can monitor its status programmatically. The job will go through `queued`, `running`, `processing_files`, and finally `completed` states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0d9e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/g_b0j0n53rz5fm8yrlw3jg040000gn/T/ipykernel_55499/2545042428.py:5: DeprecationWarning: Call to deprecated function retrieve.\n",
      "  status = together.Finetune.retrieve(fine_tune_id=job_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current job status: completed\n"
     ]
    }
   ],
   "source": [
    "# Wait til the fine tuning finish (could take a while)\n",
    "import time\n",
    "\n",
    "job_id = \"ft-338e34c5-fdc5\"\n",
    "status = together.Finetune.retrieve(fine_tune_id=job_id)\n",
    "job_status = status.get('status', 'unknown')\n",
    "print(f\"Current job status: {job_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abffd58",
   "metadata": {},
   "source": [
    "## ü§ñ 5. Use the Fine-Tuned Model for Inference\n",
    "\n",
    "Once the job is complete, your model is ready! Use the new model name returned by the API for inference. Remember to use the same prompt format (`[INST]...[/INST]`) that you used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0408ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will only work if the monitoring step above has completed successfully.\n",
    "# Get the fine-tuned model name from together fine tuning dashboard\n",
    "dedicated_endpoint = 'https://api.together.ai/v1/inference/devon_a863/llama-2-7b-chat-dolly-llama2-7b-tutorial-e305d828'  # FAKE dedicated endpoint for demonstration\n",
    "\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {os.environ.get(\"TOGETHER_API_KEY\", \"\")}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"devon_a863/llama-2-7b-chat-dolly-llama2-7b-tutorial-e305d828\",\n",
    "    \"prompt\": \"[INST] What is the secret to a successful startup? [/INST]\",\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.7,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"stop\": [\"[/INST]\", \"</s>\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(dedicated_endpoint, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    print(\"\\nFake Dedicated Endpoint Response:\")\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f53ed2",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook demonstrates the full workflow for fine-tuning a Large Language Model (LLM) using the Together AI platform:\n",
    "- **Key Concepts:** Introduces fine-tuning, dataset formatting, base models, and hyperparameters.\n",
    "- **Setup:** Installs required libraries and loads API keys securely from a `.env` file.\n",
    "- **Data Preparation:** Downloads a sample from the `databricks/databricks-dolly-15k` dataset, formats it for instruction-based fine-tuning, and saves it as a JSONL file.\n",
    "- **File Upload & Fine-Tuning:** Uploads the prepared dataset to Together AI and creates a fine-tuning job using a base model (`togethercomputer/llama-2-7b-chat`).\n",
    "- **Job Monitoring:** Shows how to monitor the fine-tuning job status programmatically.\n",
    "- **Inference:** Demonstrates how to use the fine-tuned model for inference, including handling errors and (for demonstration) how to call a fake dedicated endpoint.\n",
    "\n",
    "This notebook provides a practical, end-to-end guide for customizing LLMs with your own data on Together AI, including robust error handling and troubleshooting tips."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
