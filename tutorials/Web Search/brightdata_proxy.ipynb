{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bright Data: Robust Web Scraping with Proxy Service \n",
    "\n",
    "Welcome to this tutorial on using **[Bright Data's Proxy Network](https://get.brightdata.com/aibuilders)** with Python. While Bright Data offers high-level tools like Scraper APIs and Datasets, this guide focuses on the foundational service: the proxy network itself. \n",
    "\n",
    "We'll cover how to integrate Bright Data's powerful proxies with the popular `requests` library to perform common web scraping tasks reliably and efficiently. You'll learn how to overcome IP blocks, access content from different countries, and manage request sessions. Let's get started! üåç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Use a Proxy for Web Scraping?\n",
    "\n",
    "When you scrape a website, you send many requests from your computer's IP address. Websites can easily detect this unusual activity and may block your IP, show you misleading information (cloaking), or require you to solve CAPTCHAs. \n",
    "\n",
    "A proxy server acts as an intermediary. It forwards your request to the target website using its own IP address, hiding yours. A proxy network, like Bright Data's, gives you access to a massive pool of different IPs (Datacenter, Residential, ISP, Mobile) around the globe. This allows you to:\n",
    "\n",
    "- **Avoid IP Bans and Rate Limits**: By rotating through different IPs, your requests appear to come from many different users, making your scraper much harder to detect and block.\n",
    "- **Access Geo-Restricted Content**: You can make your request appear as if it's coming from a specific country, allowing you to scrape localized pricing, content, or services.\n",
    "- **Improve Scalability and Reliability**: A large, reliable proxy network ensures your scraper can run at scale with a high success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Installing Libraries\n",
    "\n",
    "First, we'll install the `requests` library to make HTTP requests and `python-dotenv` to manage our API credentials securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Get Your Bright Data Proxy Credentials\n",
    "\n",
    "Before we start, you need your proxy credentials from the Bright Data dashboard.\n",
    "\n",
    "1.  **Sign Up**: Create an account at **[Bright Data](https://get.brightdata.com/aibuilders)**.\n",
    "2.  **Navigate to Proxies & Scraping Infrastructure**: In the dashboard, go to this section and click \"Add\" to create a new proxy zone.\n",
    "3.  **Choose a Network Type**: For most web scraping tasks, **Residential Proxies** are the most effective. Select it and configure your zone.\n",
    "4.  **Get Credentials**: Once the zone is created, click on it and go to the \"Access parameters\" tab. You will find your **Host**, **Port**, **Username**, and **Password**. The host will typically be `brd.superproxy.io`.\n",
    "\n",
    "### 2.3. Configure Your `.env` File\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook. Storing credentials here keeps them secure and out of your code. Add your credentials like this, replacing the placeholder values with your actual access parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIGHTDATA_HOST='brd.superproxy.io'\n",
    "BRIGHTDATA_PORT='your_port'\n",
    "BRIGHTDATA_USERNAME='brd-customer-hl_xxxxxxxx-zone-your_zone_name'\n",
    "BRIGHTDATA_PASSWORD='your_zone_password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Proxy credentials loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve credentials\n",
    "host = os.getenv(\"BRIGHTDATA_HOST\")\n",
    "port = os.getenv(\"BRIGHTDATA_PORT\")\n",
    "username = os.getenv(\"BRIGHTDATA_USERNAME\")\n",
    "password = os.getenv(\"BRIGHTDATA_PASSWORD\")\n",
    "\n",
    "# Check if all credentials are loaded\n",
    "if not all([host, port, username, password]):\n",
    "    raise ValueError(\"Proxy credentials not found in .env file. Please check your configuration.\")\n",
    "    \n",
    "# Construct the proxy URL for the requests library\n",
    "proxy_url = f\"http://{username}:{password}@{host}:{port}\"\n",
    "proxies = {\n",
    "    \"http\": proxy_url,\n",
    "    \"https\": proxy_url\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Proxy credentials loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making a Basic Proxied Request\n",
    "\n",
    "Let's test our setup. We will make a request to `https://geo.brdtest.com/mygeo.json`, a Bright Data service that returns geo-location details about the IP address of the incoming request. First, we'll check our real IP, and then we'll make the same request through the proxy to see the IP change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'Singapore',\n",
       " 'region': '',\n",
       " 'region_name': '',\n",
       " 'postal_code': '31',\n",
       " 'latitude': 1.3352,\n",
       " 'longitude': 103.8529,\n",
       " 'tz': 'Asia/Singapore',\n",
       " 'lum_city': 'singapore'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_url = 'https://geo.brdtest.com/mygeo.json'\n",
    "\n",
    "response_local = requests.get(target_url)\n",
    "response_local.raise_for_status() # Raise an exception for bad status codes\n",
    "local_ip = response_local.json().get('geo')\n",
    "local_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting without a proxy...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"üåç Your Local Geo: {'city': 'Singapore', 'region': '', 'region_name': '', 'postal_code': '31', 'latitude': 1.3352, 'longitude': 103.8529, 'tz': 'Asia/Singapore', 'lum_city': 'singapore'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Requesting with a Bright Data proxy...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"üïµÔ∏è Your Proxy Geo: {'city': 'London', 'region': 'ENG', 'region_name': 'England', 'postal_code': 'EC4R', 'latitude': 51.5164, 'longitude': -0.093, 'tz': 'Europe/London', 'lum_city': 'london', 'lum_region': 'eng'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "target_url = 'https://geo.brdtest.com/mygeo.json'\n",
    "\n",
    "# 1. Request with your local IP\n",
    "try:\n",
    "    print(\"Requesting without a proxy...\")\n",
    "    response_local = requests.get(target_url)\n",
    "    response_local.raise_for_status() # Raise an exception for bad status codes\n",
    "    local_data = response_local.json().get('geo')\n",
    "    display(f\"üåç Your Local Geo: {local_data}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Request with the Bright Data proxy\n",
    "try:\n",
    "    print(\"Requesting with a Bright Data proxy...\")\n",
    "    # The 'proxies' dict tells requests to route the call through the proxy.\n",
    "    # `verify=False` is used here like the -k flag in cURL to handle SSL certificates via the proxy.\n",
    "    response_proxy = requests.get(target_url, proxies=proxies, verify=False)\n",
    "    response_proxy.raise_for_status()\n",
    "    proxy_data = response_proxy.json().get('geo')\n",
    "    display(f\"üïµÔ∏è Your Proxy Geo: {proxy_data}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Usage: Geo-Targeting\n",
    "\n",
    "One of the most powerful features of a proxy network is geo-targeting. You can make requests appear from virtually any country. With Bright Data, this is done by adding a country parameter to your proxy username.\n",
    "\n",
    "The format is `username-country-COUNTRYCODE`. For example, to use a German IP, your username would become `your_username-country-de`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting with a proxy from country: de...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  -> üïµÔ∏è Proxy Data: {'city': 'D√ºsseldorf', 'region': 'NW', 'region_name': 'North Rhine-Westphalia', 'postal_code': '40468', 'latitude': 51.2562, 'longitude': 6.7827, 'tz': 'Europe/Berlin', 'lum_city': 'dusseldorf', 'lum_region': 'nw'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting with a proxy from country: ca...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  -> üïµÔ∏è Proxy Data: {'city': 'Mississauga', 'region': 'ON', 'region_name': 'Ontario', 'postal_code': 'L5A', 'latitude': 43.5873, 'longitude': -79.614, 'tz': 'America/Toronto', 'lum_city': 'mississauga', 'lum_region': 'on'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_proxied_ip(country_code=None):\n",
    "    \"\"\"Fetches the origin IP through a potentially geo-targeted proxy.\"\"\"\n",
    "    \n",
    "    proxy_user_geo = username\n",
    "    if country_code:\n",
    "        proxy_user_geo += f\"-country-{country_code.lower()}\"\n",
    "    \n",
    "    geo_proxy_url = f\"http://{proxy_user_geo}:{password}@{host}:{port}\"\n",
    "    geo_proxies = {\n",
    "        'http': geo_proxy_url,\n",
    "        'https': geo_proxy_url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Requesting with a proxy from country: {country_code or 'Any (Rotating)'}...\")\n",
    "        response = requests.get(target_url, proxies=geo_proxies, timeout=10, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        display(f\"  -> üïµÔ∏è Proxy Data: {data.get('geo')}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  -> An error occurred: {e}\\n\")\n",
    "\n",
    "# Example: Request from Germany (DE) and Canada (CA)\n",
    "get_proxied_ip(country_code=\"de\")\n",
    "get_proxied_ip(country_code=\"ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Usage: Sticky Sessions\n",
    "\n",
    "By default, each request you send through a residential proxy zone might use a different IP. This is great for avoiding blocks. However, sometimes you need to maintain the **same IP** across multiple requests, for example, when navigating a multi-page form or a shopping cart.\n",
    "\n",
    "This is called a \"sticky session.\" To use one, you add a session ID parameter to your username: `username-session-SESSIONID`. The `SESSIONID` can be any random string or number you choose. All requests using the same session ID will be routed through the same IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--- Testing Sticky Session (Session ID: 648044) ---'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'  Request #1 -> querying https://api.ipify.org?format=json'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'    Returned IP: 45.185.133.250'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'  Request #2 -> querying https://api.ipify.org?format=json'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'    Returned IP: 45.185.133.250'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'‚úÖ Sticky success: All requests used the same IP.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "\n",
    "# Simplified sticky session test using a single IP echo endpoint.\n",
    "IP_ENDPOINT = \"https://api.ipify.org?format=json\"\n",
    "\n",
    "def test_session_ip(session_id: int, attempts: int = 2, timeout: int = 10):\n",
    "    \"\"\"Check whether the same proxy IP is kept across multiple requests using a session ID.\n",
    "    Args:\n",
    "        session_id: Arbitrary integer/str to pin the session.\n",
    "        attempts: How many requests to make (default 2 for a simple comparison).\n",
    "        timeout: Seconds before timing out each request.\n",
    "    \"\"\"\n",
    "    session_username = f\"{username}-session-{session_id}\"\n",
    "    session_proxy_url = f\"http://{session_username}:{password}@{host}:{port}\"\n",
    "    session_proxies = {\n",
    "        'http': session_proxy_url,\n",
    "        'https': session_proxy_url\n",
    "    }\n",
    "\n",
    "    display(f\"--- Testing Sticky Session (Session ID: {session_id}) ---\")\n",
    "    ips = []\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            display(f\"  Request #{i+1} -> querying {IP_ENDPOINT}\")\n",
    "            resp = requests.get(IP_ENDPOINT, proxies=session_proxies, timeout=timeout, verify=False)\n",
    "            resp.raise_for_status()\n",
    "            ip = resp.json().get('ip')\n",
    "            display(f\"    Returned IP: {ip}\")\n",
    "            ips.append(ip)\n",
    "        except Exception as e:\n",
    "            display(f\"    Error: {e}\")\n",
    "            ips.append(None)\n",
    "\n",
    "    # Simple evaluation\n",
    "    if len(ips) >= 2 and all(ips) and len(set(ips)) == 1:\n",
    "        display(\"‚úÖ Sticky success: All requests used the same IP.\")\n",
    "    else:\n",
    "        display(\"‚ùå Not sticky (or undetermined). IPs observed:\")\n",
    "        for idx, ip in enumerate(ips, start=1):\n",
    "            display(f\"    Attempt {idx}: {ip}\")\n",
    "        display(\"    (Different or missing IPs can mean rotation is enforced or the request failed.)\")\n",
    "\n",
    "# Run the simplified test\n",
    "random_session_id = random.randint(100000, 999999)\n",
    "test_session_ip(random_session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Conclusion\n",
    "\n",
    "You have successfully configured and used the Bright Data Proxy Network with Python! You've learned to mask your IP, target specific countries, and maintain sessions.\n",
    "\n",
    "To make your scrapers even more robust, always remember to:\n",
    "- **Set Realistic Headers**: Besides changing your IP, you should also set a `User-Agent` header to mimic a real web browser. This is a crucial step to avoid being identified as a bot.\n",
    "- **Implement Error Handling**: Network requests can fail. Always wrap your requests in `try...except` blocks to handle potential timeouts, connection errors, or bad HTTP status codes gracefully.\n",
    "- **Respect `robots.txt`**: Be a good internet citizen. Check a website's `robots.txt` file (e.g., `example.com/robots.txt`) for rules about which parts of the site should not be accessed by automated programs.\n",
    "\n",
    "This tutorial provides a solid foundation for building powerful and resilient web scrapers. To explore more advanced features or different proxy types, check out the official **[Bright Data documentation](https://get.brightdata.com/aibuilders)**.\n",
    "\n",
    "Happy scraping! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
