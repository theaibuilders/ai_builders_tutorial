{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Engineering and Context-Problem Fit: A Hands-On Introduction\n",
    "\n",
    "Welcome to this tutorial on **Context Engineering and Context-Problem Fit**! While prompt engineering crafts the perfect question for Large Language Models (LLMs), context engineering supplies the right *information* to answer it effectively. We'll also introduce the concept of Context-Problem Fit to help developers improve and optimize their LLM context to better suit their use cases.\n",
    "\n",
    "## What Is Context Engineering and Why It Matters?\n",
    "\n",
    "Context engineering is the deliberate practice of sourcing, structuring, selecting, compressing, and governing information fed to an LLM for high-accuracy, efficient, and safe problem-solving.\n",
    "\n",
    "LLMs' output is non-deterministic, hence it demands smart management to curb hallucinations, boost relevance, and handle complex tasks. Poor context engineering leads to failures like missing facts, noisy content, wasted tokens, or unreliable outputs.\n",
    "\n",
    "## Context-Problem Fit (CPF)\n",
    "Because of importance of context engineering, we introduce a new concept: **Context-Problem Fit (CPF)**.\n",
    "\n",
    "**Context-Problem Fit (CPF)** is the principle in context engineering that ensures the assembled information fed to a large language model—such as retrieved documents, system instructions, tool schemas, memory, and artifacts—is necessary, sufficient, and efficient for the specific problem that the application is solving.\n",
    "\n",
    "We can define CPF with the following heuristic index:\n",
    "\n",
    "**CPF ≈ UPR / LTR = (UX × Precision × Relevance) / (Latency × Token Cost × Risk)**\n",
    "\n",
    "You can remember them as \"Upper\" for UPR, \"Later\" for LTR becasue we want the numerators up, and \"don't let it drag later\" for denominators.\n",
    "\n",
    "Where:\n",
    "- UX: user experience of the LLM-based applications\n",
    "- Precision: noise minimized, fact checked\n",
    "- Relevance: semantic + task alignment\n",
    "- Latency: time to complete the task\n",
    "- Token Cost: total input + output tokens consumed \n",
    "- Risk: hallucination / compliance / safety exposure\n",
    "\n",
    "CPF is key to success—great agents fail more from poor fit than weak models. Always evaluate: Does this context boost the probability of a correct, useful answer? If not, refine via retrieval, compression, or filtering.\n",
    "\n",
    "Let's dive in! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Our Environment\n",
    "\n",
    "First, let's get our environment ready. We'll be using the `dotenv` library to load our API keys from a `.env` file. This is a good practice for keeping your secrets safe and out of your code. \n",
    "\n",
    "You'll also need to install the necessary libraries. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv langchain langchain-openai faiss-cpu\n",
    "```\n",
    "\n",
    "Now, create a `.env` file in the same directory as this notebook. Add your OpenAI API key to this file like so:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\"your_api_key_here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if openai_api_key:\n",
    "    print(\"API Key loaded successfully!\")\n",
    "    # Initialize the main LLM we will use throughout the tutorial\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "else:\n",
    "    print(\"API Key not found. Make sure you have a .env file with your OPENAI_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Knowledge Base: RAG with Vector Stores\n",
    "\n",
    "As we first explored, a primary form of context is an external **knowledge base**. Retrieval-Augmented Generation (RAG) is the technique of retrieving relevant data from a knowledge base (like a vector database) and providing it to the LLM. This is a foundational concept in context engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG Example ---\n",
      "Xylos has two moons.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Our knowledge source\n",
    "knowledge_base = \"\"\"\n",
    "Planet Xylos has two moons, Zylo and Nylo. The native inhabitants are called the Xylotians, a species of intelligent, six-legged creatures. They are a peaceful and technologically advanced civilization.\n",
    "\"\"\"\n",
    "\n",
    "# Create a vector store retriever from the knowledge base\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_text(knowledge_base)\n",
    "vectorstore = FAISS.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_template = \"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- RAG Example ---\")\n",
    "print(rag_chain.invoke(\"How many moons does Xylos have?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context is much more than just the RAG knowledge base. An advanced agent uses several types of context to understand its goals, capabilities, and history. Let's explore these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 System Prompt & System Metadata\n",
    "\n",
    "The **System Prompt** defines the agent's persona, high-level instructions, and constraints. **System Metadata** provides the agent with information about its own state, like the current time or message history stats. We can combine these to create a more aware agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Prompt Example ---\n",
      "- Current Time: 10:47:53\n",
      "- Conversation Length: 2 messages\n",
      "- Primary Directive: Be as concise as possible.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import datetime\n",
    "\n",
    "# Simulate metadata\n",
    "message_history = [\"user: hello\", \"assistant: hi there!\"]\n",
    "metadata = {\n",
    "    \"current_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"message_count\": len(message_history)\n",
    "}\n",
    "\n",
    "# Craft the system prompt with metadata included\n",
    "system_prompt_string = f\"\"\"\n",
    "You are a helpful assistant named 'Agent-7'. \n",
    "Your core directive is to be as concise as possible in your responses. \n",
    "\n",
    "--- System Metadata ---\n",
    "Current Time: {metadata['current_time']}\n",
    "Current Conversation Length: {metadata['message_count']} messages\n",
    "--- End Metadata ---\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_string),\n",
    "    HumanMessage(content=\"What time is it? What's the current conversation Length? What is the primary directive?\"),\n",
    "]\n",
    "\n",
    "print(\"--- System Prompt Example ---\")\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tool Schemas\n",
    "\n",
    "**Tool Schemas** are definitions of the functions or capabilities the agent can use. By providing the LLM with these schemas, it learns what actions it can perform to accomplish a task. This is a core component of making agents that can interact with the outside world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool Schema Example ---\n",
      "AI message contains a tool call:\n",
      "[{'name': 'get_current_stock_price', 'args': {'symbol': 'GOOGL'}, 'id': 'call_6a9dl7ecUZy4w665Eg1osh6r', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define a tool schema using a decorator\n",
    "@tool\n",
    "def get_current_stock_price(symbol: str) -> float:\n",
    "    \"\"\"Gets the current stock price for a given ticker symbol.\"\"\"\n",
    "    # In a real app, this would call a stock API.\n",
    "    if symbol.upper() == \"GOOGL\":\n",
    "        return 175.57\n",
    "    elif symbol.upper() == \"AAPL\":\n",
    "        return 214.29\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Bind the tool to the LLM\n",
    "llm_with_tools = llm.bind_tools([get_current_stock_price])\n",
    "\n",
    "print(\"--- Tool Schema Example ---\")\n",
    "# The LLM's response will contain a tool_call object if it decides to use the tool\n",
    "ai_msg = llm_with_tools.invoke(\"What is the stock price of Google?\")\n",
    "print(\"AI message contains a tool call:\")\n",
    "print(ai_msg.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Short-Term Memory (Message Buffer)\n",
    "\n",
    "The **Message Buffer** is the history of the conversation. It serves as the agent's short-term memory, allowing it to understand the flow of dialogue and refer back to previous points. `ConversationChain` in LangChain manages this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message Buffer / Memory Example (RunnableWithMessageHistory) ---\n",
      "\n",
      "--- Memory Buffer Contents ---\n",
      "User: My name is Bob.\n",
      "AI: Nice to meet you, Bob! How can I assist you today?\n",
      "User: What is my name?\n",
      "AI: Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "# Updated implementation replacing deprecated ConversationChain with RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# In-memory store for session histories (for demo purposes)\n",
    "session_store = {}\n",
    "\n",
    "# Function LangChain will call to get (or create) the history object for a session\n",
    "def get_session_history(session_id: str):\n",
    "    return session_store.setdefault(session_id, ChatMessageHistory())\n",
    "\n",
    "# Build a prompt that explicitly includes a placeholder for past messages\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise assistant that remembers the conversation.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Base chain (prompt -> llm)\n",
    "base_chain = prompt | llm\n",
    "\n",
    "# Wrap with message history support\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",      # key in invoke() input dict for new user message\n",
    "    history_messages_key=\"history\"    # name of the placeholder in the prompt\n",
    ")\n",
    "\n",
    "print(\"--- Message Buffer / Memory Example (RunnableWithMessageHistory) ---\")\n",
    "\n",
    "session_id = \"demo-session\"\n",
    "\n",
    "# First user turn\n",
    "response_1 = chain_with_history.invoke(\n",
    "    {\"input\": \"My name is Bob.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "# print(\"User: My name is Bob.\")\n",
    "# print(f\"AI: {response_1}\")\n",
    "\n",
    "# Second user turn referring to prior context\n",
    "response_2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "# print(\"User: What is my name?\")\n",
    "# print(f\"AI: {response_2}\")\n",
    "\n",
    "# Inspect the stored history messages\n",
    "print(\"\\n--- Memory Buffer Contents ---\")\n",
    "for m in session_store[session_id].messages:\n",
    "    role = 'User' if isinstance(m, HumanMessage) else 'AI'\n",
    "    print(f\"{role}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Long-Term Memory Systems\n",
    "Short-term message buffers saturate quickly. Long-term memory stores salient, reusable facts or user preferences outside the token window and rehydrates only what is relevant.\n",
    "\n",
    "Core ideas:\n",
    "- Persist episodic facts (who, what, when) + semantic summaries.\n",
    "- Retrieve by hybrid signals: embedding similarity + recency + importance.\n",
    "- Apply write policies: only store if novel & likely reusable.\n",
    "\n",
    "Design Checklist:\n",
    "- Memory Types: profile (static), episodic (events), semantic (summaries), procedural (how-to steps).\n",
    "- Indexing: vector DB (semantic), key-value (IDs), time-sorted logs.\n",
    "\n",
    "We'll simulate a tiny memory store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved memory context:\n",
      "- User prefers concise answers\n",
      "- User name is Bob\n",
      "- User likes sci-fi metaphors\n",
      "\n",
      "Augmented Answer:\n",
      " Greetings, Bob! Ready to launch into another day of exploration in the vast universe of knowledge? Let’s make it a stellar journey!\n"
     ]
    }
   ],
   "source": [
    "# Long-Term Memory Demo (lightweight, in-notebook simulation)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "import math, time\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "@dataclass\n",
    "class MemoryItem:\n",
    "    text: str\n",
    "    ts: float\n",
    "    importance: float  # heuristic 0-1\n",
    "    embedding: List[float] = field(default_factory=list)\n",
    "\n",
    "class LongTermMemory:\n",
    "    def __init__(self, embed_model=None):\n",
    "        self.items: List[MemoryItem] = []\n",
    "        self.embed_model = embed_model or OpenAIEmbeddings()\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        return self.embed_model.embed_query(text)\n",
    "\n",
    "    def maybe_store(self, text: str, importance: float):\n",
    "        # Simple novelty gate: skip if very similar to an existing memory\n",
    "        emb = self._embed(text)\n",
    "        for it in self.items:\n",
    "            # cosine similarity\n",
    "            dot = sum(a*b for a,b in zip(emb, it.embedding))\n",
    "            norm_a = math.sqrt(sum(a*a for a in emb))\n",
    "            norm_b = math.sqrt(sum(a*a for a in it.embedding))\n",
    "            sim = dot/(norm_a*norm_b + 1e-9)\n",
    "            if sim > 0.92:\n",
    "                return False\n",
    "        self.items.append(MemoryItem(text=text, ts=time.time(), importance=importance, embedding=emb))\n",
    "        return True\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3, alpha_recency=0.15, beta_importance=0.3):\n",
    "        q_emb = self._embed(query)\n",
    "        scored = []\n",
    "        now = time.time()\n",
    "        for it in self.items:\n",
    "            dot = sum(a*b for a,b in zip(q_emb, it.embedding))\n",
    "            norm_a = math.sqrt(sum(a*a for a in q_emb))\n",
    "            norm_b = math.sqrt(sum(a*a for a in it.embedding))\n",
    "            sim = dot/(norm_a*norm_b + 1e-9)\n",
    "            recency = math.exp(- (now - it.ts)/3600)  # decays over hours\n",
    "            score = sim + alpha_recency*recency + beta_importance*it.importance\n",
    "            scored.append((score, it))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [it.text for score,it in scored[:k]]\n",
    "\n",
    "ltm = LongTermMemory()\n",
    "ltm.maybe_store(\"User prefers concise answers\", importance=0.9)\n",
    "ltm.maybe_store(\"User name is Bob\", importance=0.7)\n",
    "ltm.maybe_store(\"User likes sci-fi metaphors\", importance=0.6)\n",
    "\n",
    "query = \"What's the user's name and style preferences?\"\n",
    "mem_ctx = ltm.retrieve(query)\n",
    "\n",
    "print(\"Retrieved memory context:\")\n",
    "for m in mem_ctx:\n",
    "    print(\"-\", m)\n",
    "\n",
    "# Feed retrieved memory into an augmented prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "mem_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant. Incorporate the following persistent user memory facts if relevant:\\n{memory}\\n\\nUser Query: {question}\\nAnswer:\"\"\"\n",
    ")\n",
    "mem_chain = mem_prompt | llm | StrOutputParser()\n",
    "print(\"\\nAugmented Answer:\\n\", mem_chain.invoke({\"memory\": \"\\n\".join(mem_ctx), \"question\": \"Could you greet the user appropriately?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Scratchpads / Working Memory\n",
    "Maintain explicit intermediate reasoning artifacts (plans, hypotheses, partial results) separate from user-visible answers. Feed refined summaries forward; drop raw chains when unnecessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan Only (store as scratchpad artifact):\n",
      " PLAN:\n",
      "\n",
      "1. **Day 1: Arrival in Marseille**\n",
      "   - Morning: Arrive in Marseille, check into a hotel.\n",
      "   - Afternoon: Visit the Musée des Civilisations de l'Europe et de la Méditerranée (MuCEM) for a cultural introduction.\n",
      "   - Evening: Stroll through the Vieux-Port and enjoy dinner at a local seafood restaurant.\n",
      "\n",
      "2. **Day 2: Aix-en-Provence**\n",
      "   - Morning: Take a train to Aix-en-Provence (approx. 30 minutes).\n",
      "   - Afternoon: Explore the Atelier Cézanne and the Musée Granet to appreciate local art.\n",
      "   - Evening: Walk through the Cours Mirabeau and dine at a traditional Provençal restaurant.\n",
      "\n",
      "3. **Day 3: Arles**\n",
      "   - Morning: Travel by train to Arles (approx. 1 hour).\n",
      "   - Afternoon: Visit the Fondation Vincent van Gogh and the Roman Amphitheatre.\n",
      "   - Evening: Enjoy a riverside dinner along the Rhône.\n",
      "\n",
      "4. **Day 4: Nice**\n",
      "   - Morning: Take a train to Nice (approx. 3 hours).\n",
      "   - Afternoon: Explore the Promenade des Anglais and the Musée Matisse.\n",
      "   - Evening: Relax on the beach and have dinner at a coastal restaurant.\n",
      "\n",
      "\n",
      "\n",
      "User-Facing Answer:\n",
      " To organize a 4-day train itinerary in southern France focusing on art and the coast, start in Marseille, exploring its cultural sites and enjoying seafood by the Vieux-Port. On the second day, head to Aix-en-Provence to visit art museums and enjoy the charming streets. The third day takes you to Arles, where you can immerse yourself in Van Gogh's legacy and Roman history. Conclude your journey in Nice, where you can enjoy the coastal views and visit the Musée Matisse. This itinerary offers a blend of art, history, and coastal relaxation, all accessible by train.\n"
     ]
    }
   ],
   "source": [
    "# Scratchpad simulation\n",
    "reasoning_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a planner. Given a user goal, first produce a structured PLAN with numbered steps in 200 words, then output a FINAL answer.\\nGoal: {goal}\\n\\nRespond using:\\nPLAN:\\n1. ...\\n2. ...\\nFINAL:\\n...\"\"\"\n",
    ")\n",
    "resp = (reasoning_prompt | llm | StrOutputParser()).invoke({\"goal\": \"Organize a 4 day train itinerary southern France: art + coast\"})\n",
    "# Split plan vs final\n",
    "plan_section, final_section = resp.split(\"FINAL:\", 1) if \"FINAL:\" in resp else (resp, \"\")\n",
    "print(\"Plan Only (store as scratchpad artifact):\\n\", plan_section)\n",
    "print(\"\\nUser-Facing Answer:\\n\", final_section.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Files & Artifacts\n",
    "\n",
    "This involves providing the agent with direct access to **Files & Artifacts** like PDFs, CSVs, or code files. This is similar to RAG but can be more direct, where the entire content of a file is loaded into the context to be summarized, analyzed, or transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files & Artifacts Example ---\n",
      "The system must support up to 100 concurrent users.\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a dummy file to act as our artifact\n",
    "file_content = \"\"\"\n",
    "PROJECT REQUIREMENTS DOCUMENT\n",
    "1. The user interface must be blue.\n",
    "2. The login button must be labeled 'Enter'.\n",
    "3. The system must support up to 100 concurrent users.\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(file_content)\n",
    "\n",
    "# Now, load the file content\n",
    "with open(\"requirements.txt\", \"r\") as f:\n",
    "    file_context = f.read()\n",
    "\n",
    "file_prompt_template = \"\"\"\n",
    "You are a project manager assistant. Answer the user's question based on the following project requirements file.\n",
    "\n",
    "--- File Content: requirements.txt ---\n",
    "{file_context}\n",
    "--- End File Content ---\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "\n",
    "file_prompt = ChatPromptTemplate.from_template(file_prompt_template)\n",
    "\n",
    "file_chain = file_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"--- Files & Artifacts Example ---\")\n",
    "response = file_chain.invoke({\n",
    "    \"file_context\": file_context,\n",
    "    \"user_question\": \"How many users must the system support?\"\n",
    "})\n",
    "print(response)\n",
    "\n",
    "# Clean up the dummy file\n",
    "os.remove(\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Tool Outputs as Context\n",
    "After a tool call, its result becomes new context. Decide: inline raw output, summarize, or index for later retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you should pack a light jacket for evenings in Nice, as the lows are around 16°C, which can be a bit cool.\n"
     ]
    }
   ],
   "source": [
    "# Tool output integration demo (reuse earlier stock tool pattern)\n",
    "# Simulate calling a weather API (stub)\n",
    "import random\n",
    "\n",
    "def fake_weather(city: str):\n",
    "    return {\n",
    "        'city': city,\n",
    "        'forecast': 'sunny',\n",
    "        'high_c': 24 + random.randint(-2,2),\n",
    "        'low_c': 16 + random.randint(-2,2)\n",
    "    }\n",
    "\n",
    "weather_data = fake_weather('Nice, France')\n",
    "weather_context = f\"Weather for {weather_data['city']}: {weather_data['forecast']} highs {weather_data['high_c']}C lows {weather_data['low_c']}C\"\n",
    "\n",
    "weather_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are planning a trip. Incorporate the provided tool result succinctly if relevant.\\nTOOL RESULT:\\n{tool}\\n\\nUser Question: {q}\\nAnswer:\"\"\"\n",
    ")\n",
    "print((weather_prompt | llm | StrOutputParser()).invoke({\n",
    "    'tool': weather_context,\n",
    "    'q': 'Should I pack a light jacket for evenings?'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example CPF Optimization Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Summarization & Compression\n",
    "Purpose: keep salient information while shedding token weight. Prevents context bloat and preserves CPF at scale.\n",
    "\n",
    "Heuristic Trigger Examples:\n",
    "- Token threshold exceeded.\n",
    "- Conversation phase shift (new task detected).\n",
    "- Redundancy score high (n-gram overlap, embedding near-duplicates).\n",
    "\n",
    "CPF Impact:\n",
    "- **Increases**: UX, reduces repetition and cognitive load; the model anchors on a clean distilled state so answers feel sharper and more context-aware.\n",
    "- **Decreases**: Token Cost, Consolidates many historical turns into compact summaries.\n",
    "\n",
    "Below we simulate a rolling summary policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Rolling Summary:\n",
      " The user is exploring building a travel planner focused on coastal cities in Europe. They prefer traveling by train over flights and aim to keep trips under 5 days with a moderate budget.\n",
      "Active Buffer:\n",
      "[ROLLED: summary refreshed]\n"
     ]
    }
   ],
   "source": [
    "# Rolling summary demo\n",
    "from collections import deque\n",
    "\n",
    "turns = deque(maxlen=12)\n",
    "summary = \"\"\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a summarizer. Given an existing summary and new dialogue turns, produce an updated concise summary preserving key facts and user preferences.\\n\\nExisting Summary (may be empty):\\n{existing}\\n\\nNew Turns:\\n{new_turns}\\n\\nUpdated Summary:\"\"\"\n",
    ")\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "def add_turn(speaker, text):\n",
    "    global summary\n",
    "    turns.append(f\"{speaker}: {text}\")\n",
    "    # If raw turns token estimate > threshold (crude: char count) -> summarize\n",
    "    raw_context = \"\\n\".join(turns)\n",
    "    if len(raw_context) > 280:  # pretend token heuristic\n",
    "        summary = summary_chain.invoke({\"existing\": summary, \"new_turns\": raw_context})\n",
    "        turns.clear()\n",
    "        turns.append(\"[ROLLED: summary refreshed]\")\n",
    "\n",
    "add_turn(\"User\", \"Hi, I'm exploring building a travel planner.\")\n",
    "add_turn(\"AI\", \"Great! What destinations interest you?\")\n",
    "add_turn(\"User\", \"Mostly coastal cities in Europe, and I prefer trains over flights.\")\n",
    "add_turn(\"AI\", \"Noted. Budget or time constraints?\")\n",
    "add_turn(\"User\", \"Keep trips under 5 days each, moderate budget.\")\n",
    "add_turn(\"AI\", \"Understood. I'll remember: coastal Europe, trains, <5 days, moderate budget.\")\n",
    "\n",
    "print(\"Current Rolling Summary:\\n\", summary)\n",
    "print(\"Active Buffer:\\n\" + \"\\n\".join(turns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pruning & Relevance Management\n",
    "Keep only what moves the answer quality needle. Pruning reduces noise & latency.\n",
    "\n",
    "Signals for pruning:\n",
    "- Low retrieval score across recent queries.\n",
    "- Age decay below threshold & not re-referenced.\n",
    "- Conflicts with newer authoritative facts.\n",
    "\n",
    "Strategy: maintain metadata (last_access, access_count, decay score). Periodically purge or archive.\n",
    "\n",
    "CPF Impact:\n",
    "- **Increases**: Relevance, Removes context with low recent utility or semantic mismatch; Precision, Eliminates stale/conflicting facts improving answer consistency.\n",
    "- **Decreases**: Token Cost, Shrinks context window footprint over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned IDs: [1, 3]\n",
      "Remaining texts:\n",
      "- Critical user profile: prefers trains\n"
     ]
    }
   ],
   "source": [
    "# Simple pruning policy demonstration\n",
    "import random, time\n",
    "class PrunableStore:\n",
    "    def __init__(self):\n",
    "        self.rows: Dict[int, Dict[str, Any]] = {}\n",
    "        self._id = 0\n",
    "    def add(self, text, importance):\n",
    "        self._id += 1\n",
    "        self.rows[self._id] = {\n",
    "            'text': text,\n",
    "            'importance': importance,\n",
    "            'last_access': time.time(),\n",
    "            'access_count': 0\n",
    "        }\n",
    "    def access(self, rid):\n",
    "        if rid in self.rows:\n",
    "            self.rows[rid]['last_access'] = time.time()\n",
    "            self.rows[rid]['access_count'] += 1\n",
    "            return self.rows[rid]['text']\n",
    "    def prune(self, min_importance=0.3, max_age=120, min_access=1):\n",
    "        now = time.time()\n",
    "        to_delete = []\n",
    "        for rid, row in self.rows.items():\n",
    "            age = now - row['last_access']\n",
    "            if (row['importance'] < min_importance and age > max_age) or (row['access_count'] < min_access and age > max_age*0.5):\n",
    "                to_delete.append(rid)\n",
    "        for rid in to_delete:\n",
    "            del self.rows[rid]\n",
    "        return to_delete\n",
    "\n",
    "store = PrunableStore()\n",
    "store.add(\"Transient debugging detail: foo var changed\", importance=0.1)\n",
    "store.add(\"Critical user profile: prefers trains\", importance=0.9)\n",
    "store.add(\"Old ephemeral greeting\", importance=0.2)\n",
    "# Simulate time passage\n",
    "for rid in list(store.rows.keys()):\n",
    "    if store.rows[rid]['importance'] > 0.5:\n",
    "        store.access(rid)  # mark as used\n",
    "# Fake aging\n",
    "for row in store.rows.values():\n",
    "    row['last_access'] -= 400\n",
    "\n",
    "removed = store.prune()\n",
    "print(\"Pruned IDs:\", removed)\n",
    "print(\"Remaining texts:\")\n",
    "for r in store.rows.values():\n",
    "    print(\"-\", r['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Few-Shot Example Selection\n",
    "Instead of static examples, dynamically retrieve the most similar exemplars to the current query to maximize relevance while staying within token budgets.\n",
    "\n",
    "CPF Impact:\n",
    "- **Increases**: Relevance, Selects semantically proximate exemplars guiding style & structure; UX, Users perceive higher personalization \n",
    "- **Decreases**: Token Cost, Limits to top-k exemplars vs. a large static block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-day itinerary: Nice, Antibes, Marseille; art museums & coastal views.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot dynamic selection\n",
    "from langchain_community.vectorstores import FAISS as FVS\n",
    "examples = [\n",
    "    {\"input\": \"Plan a 3 day train trip to Paris focused on art.\", \"output\": \"3-day Paris itinerary with Louvre, Orsay, Pompidou.\"},\n",
    "    {\"input\": \"Plan a 5 day coastal Spain itinerary.\", \"output\": \"Include Barcelona, Valencia; trains between cities.\"},\n",
    "    {\"input\": \"Suggest 4 days in Amsterdam with museums.\", \"output\": \"Rijksmuseum, Van Gogh, canal tour, bike day.\"},\n",
    "]\n",
    "emb_model = OpenAIEmbeddings()\n",
    "exa_texts = [e['input'] for e in examples]\n",
    "exa_store = FVS.from_texts(exa_texts, emb_model, metadatas=examples)\n",
    "\n",
    "user_request = \"Give me a 4 day art & coast train itinerary in southern France\"\n",
    "retrieved = exa_store.similarity_search(user_request, k=2)\n",
    "\n",
    "fewshot_block = \"\\n\\n\".join([f\"Example:\\nUser: {r.metadata['input']}\\nAssistant: {r.metadata['output']}\" for r in retrieved])\n",
    "\n",
    "fewshot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a travel assistant. Use the style of the examples.\\n{examples}\\n\\nUser Request: {query}\\nResponse:\"\"\"\n",
    ")\n",
    "print((fewshot_prompt | llm | StrOutputParser()).invoke({\"examples\": fewshot_block, \"query\": user_request})[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Guardrails & Validation\n",
    "Validate or sanitize context & outputs before surfacing or acting. Useful for safety, correctness, and CPF hygiene.\n",
    "\n",
    "CPF Impact:\n",
    "- **Increases**: UX, Builds trust—consistent, safe responses improve perceived reliability.\n",
    "- **Decreases**: Risk, Directly mitigates safety, compliance, and hallucination exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: No, you should not bring classified documents on vacation.\n",
      "Validated: [REDACTED: disallowed content detected]\n"
     ]
    }
   ],
   "source": [
    "# Simple validation wrapper: enforce max hallucination risk keywords\n",
    "FORBIDDEN = {\"nuclear\", \"classified\"}\n",
    "\n",
    "def validate_output(text: str):\n",
    "    tokens = set(text.lower().split())\n",
    "    if tokens & FORBIDDEN:\n",
    "        return \"[REDACTED: disallowed content detected]\"\n",
    "    return text\n",
    "\n",
    "raw_answer = (ChatPromptTemplate.from_template(\n",
    "    \"Answer succinctly: {q}\" ) | llm | StrOutputParser()).invoke({\"q\": \"Should I bring any classified documents on vacation?\"})\n",
    "print(\"Original:\", raw_answer)\n",
    "print(\"Validated:\", validate_output(raw_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now walked through a practical, end‑to‑end snapshot of modern Context Engineering. We moved from a single retrieval step (basic RAG) to a layered pipeline that deliberately curates, shapes, and governs what an LLM sees. The goal throughout: maximize Context‑Problem Fit (CPF).\n",
    "\n",
    "Think of context as a living knowledge substrate where each token slot is a scarce budget line. Your job: continuously re-balance the portfolio of facts, summaries, exemplars, and tool results so that the marginal token delivers maximal probability of a correct, safe, and useful answer.\n",
    "\n",
    "Keep iterating: instrument, measure, prune, compress, and re-verify. That cycle is the heart of context engineering.\n",
    "\n",
    "Happy building! 🛠️🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
