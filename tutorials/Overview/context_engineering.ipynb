{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Engineering: A Hands-on Introduction\n",
    "\n",
    "Welcome to this tutorial on **Context Engineering**! In the world of Large Language Models (LLMs), you've probably heard of Prompt Engineering. While prompt engineering is about crafting the perfect question, context engineering is about providing the LLM with the right *information* to answer that question. It's the art and science of designing systems that feed LLMs the relevant context they need to perform a task effectively. \n",
    "\n",
    "Think of it like this: if you ask a historian a question, they'll give you a much better answer if they have their library of books and research papers handy. Context engineering is about building that library for your LLM and making sure it can find the right book at the right time. \n",
    "\n",
    "## Why is Context Engineering Important?\n",
    "\n",
    "LLMs have a limited \"memory\" called the **context window**. They can only consider a certain amount of information at a time. Context engineering helps us make the most of this limited space by:\n",
    "\n",
    "* **Improving Accuracy:** By providing relevant, factual information, we can reduce the chances of the LLM \"hallucinating\" or making things up.\n",
    "* **Enhancing Relevance:** We can steer the LLM's responses to be more specific and tailored to the user's needs.\n",
    "* **Enabling Complex Tasks:** For tasks that require knowledge of specific documents or conversations, context engineering is essential. \n",
    "\n",
    "### Context-Problem Fit (CPF)\n",
    "A core idea you'll rely on throughout this notebook is achieving strong **Context-Problem Fit (CPF)**. CPF means the context you assemble (retrieved documents, system instructions, tool schemas, memory, artifacts, etc.) is:\n",
    "\n",
    "1. **Necessary** ‚Äî each piece directly supports solving the current task.\n",
    "2. **Sufficient** ‚Äî taken together, the context eliminates major ambiguity and minimizes guesswork.\n",
    "3. **Efficient** ‚Äî it fits within token limits without bloating the prompt with redundant or low-signal content.\n",
    "\n",
    "Great agents fail more often from poor CPF than from weak model capability. Always ask: *Does this context chunk increase the probability of a correct, useful answer for this specific problem?* If not, refine retrieval, compress, filter, or enrich.\n",
    "\n",
    "This notebook will walk you through the basics of context engineering, from a simple Retrieval-Augmented Generation (RAG) system to System Prompts, Tool Schemas, and Message Buffer, and Files & Artifacts etc. Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Our Environment\n",
    "\n",
    "First, let's get our environment ready. We'll be using the `dotenv` library to load our API keys from a `.env` file. This is a good practice for keeping your secrets safe and out of your code. \n",
    "\n",
    "You'll also need to install the necessary libraries. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv langchain langchain-openai faiss-cpu\n",
    "```\n",
    "\n",
    "Now, create a `.env` file in the same directory as this notebook. Add your OpenAI API key to this file like so:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\"your_api_key_here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if openai_api_key:\n",
    "    print(\"API Key loaded successfully!\")\n",
    "    # Initialize the main LLM we will use throughout the tutorial\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "else:\n",
    "    print(\"API Key not found. Make sure you have a .env file with your OPENAI_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Knowledge Base: RAG with Vector Stores\n",
    "\n",
    "As we first explored, a primary form of context is an external **knowledge base**. Retrieval-Augmented Generation (RAG) is the technique of retrieving relevant data from a knowledge base (like a vector database) and providing it to the LLM. This is a foundational concept in context engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG Example ---\n",
      "Xylos has two moons.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Our knowledge source\n",
    "knowledge_base = \"\"\"\n",
    "Planet Xylos has two moons, Zylo and Nylo. The native inhabitants are called the Xylotians, a species of intelligent, six-legged creatures. They are a peaceful and technologically advanced civilization.\n",
    "\"\"\"\n",
    "\n",
    "# Create a vector store retriever from the knowledge base\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_text(knowledge_base)\n",
    "vectorstore = FAISS.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_template = \"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- RAG Example ---\")\n",
    "print(rag_chain.invoke(\"How many moons does Xylos have?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context is much more than just the RAG knowledge base. An advanced agent uses several types of context to understand its goals, capabilities, and history. Let's explore these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 System Prompt & System Metadata\n",
    "\n",
    "The **System Prompt** defines the agent's persona, high-level instructions, and constraints. **System Metadata** provides the agent with information about its own state, like the current time or message history stats. We can combine these to create a more aware agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Prompt Example ---\n",
      "- Current Time: 10:47:53\n",
      "- Conversation Length: 2 messages\n",
      "- Primary Directive: Be as concise as possible.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import datetime\n",
    "\n",
    "# Simulate metadata\n",
    "message_history = [\"user: hello\", \"assistant: hi there!\"]\n",
    "metadata = {\n",
    "    \"current_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"message_count\": len(message_history)\n",
    "}\n",
    "\n",
    "# Craft the system prompt with metadata included\n",
    "system_prompt_string = f\"\"\"\n",
    "You are a helpful assistant named 'Agent-7'. \n",
    "Your core directive is to be as concise as possible in your responses. \n",
    "\n",
    "--- System Metadata ---\n",
    "Current Time: {metadata['current_time']}\n",
    "Current Conversation Length: {metadata['message_count']} messages\n",
    "--- End Metadata ---\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_string),\n",
    "    HumanMessage(content=\"What time is it? What's the current conversation Length? What is the primary directive?\"),\n",
    "]\n",
    "\n",
    "print(\"--- System Prompt Example ---\")\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tool Schemas\n",
    "\n",
    "**Tool Schemas** are definitions of the functions or capabilities the agent can use. By providing the LLM with these schemas, it learns what actions it can perform to accomplish a task. This is a core component of making agents that can interact with the outside world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool Schema Example ---\n",
      "AI message contains a tool call:\n",
      "[{'name': 'get_current_stock_price', 'args': {'symbol': 'GOOGL'}, 'id': 'call_6a9dl7ecUZy4w665Eg1osh6r', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define a tool schema using a decorator\n",
    "@tool\n",
    "def get_current_stock_price(symbol: str) -> float:\n",
    "    \"\"\"Gets the current stock price for a given ticker symbol.\"\"\"\n",
    "    # In a real app, this would call a stock API.\n",
    "    if symbol.upper() == \"GOOGL\":\n",
    "        return 175.57\n",
    "    elif symbol.upper() == \"AAPL\":\n",
    "        return 214.29\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Bind the tool to the LLM\n",
    "llm_with_tools = llm.bind_tools([get_current_stock_price])\n",
    "\n",
    "print(\"--- Tool Schema Example ---\")\n",
    "# The LLM's response will contain a tool_call object if it decides to use the tool\n",
    "ai_msg = llm_with_tools.invoke(\"What is the stock price of Google?\")\n",
    "print(\"AI message contains a tool call:\")\n",
    "print(ai_msg.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Message Buffer (Short-Term Memory)\n",
    "\n",
    "The **Message Buffer** is the history of the conversation. It serves as the agent's short-term memory, allowing it to understand the flow of dialogue and refer back to previous points. `ConversationChain` in LangChain manages this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Message Buffer / Memory Example (RunnableWithMessageHistory) ---\n",
      "\n",
      "--- Memory Buffer Contents ---\n",
      "User: My name is Bob.\n",
      "AI: Nice to meet you, Bob! How can I assist you today?\n",
      "User: What is my name?\n",
      "AI: Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "# Updated implementation replacing deprecated ConversationChain with RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# In-memory store for session histories (for demo purposes)\n",
    "session_store = {}\n",
    "\n",
    "# Function LangChain will call to get (or create) the history object for a session\n",
    "def get_session_history(session_id: str):\n",
    "    return session_store.setdefault(session_id, ChatMessageHistory())\n",
    "\n",
    "# Build a prompt that explicitly includes a placeholder for past messages\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise assistant that remembers the conversation.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Base chain (prompt -> llm)\n",
    "base_chain = prompt | llm\n",
    "\n",
    "# Wrap with message history support\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",      # key in invoke() input dict for new user message\n",
    "    history_messages_key=\"history\"    # name of the placeholder in the prompt\n",
    ")\n",
    "\n",
    "print(\"--- Message Buffer / Memory Example (RunnableWithMessageHistory) ---\")\n",
    "\n",
    "session_id = \"demo-session\"\n",
    "\n",
    "# First user turn\n",
    "response_1 = chain_with_history.invoke(\n",
    "    {\"input\": \"My name is Bob.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "# print(\"User: My name is Bob.\")\n",
    "# print(f\"AI: {response_1}\")\n",
    "\n",
    "# Second user turn referring to prior context\n",
    "response_2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "# print(\"User: What is my name?\")\n",
    "# print(f\"AI: {response_2}\")\n",
    "\n",
    "# Inspect the stored history messages\n",
    "print(\"\\n--- Memory Buffer Contents ---\")\n",
    "for m in session_store[session_id].messages:\n",
    "    role = 'User' if isinstance(m, HumanMessage) else 'AI'\n",
    "    print(f\"{role}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Files & Artifacts\n",
    "\n",
    "This involves providing the agent with direct access to **Files & Artifacts** like PDFs, CSVs, or code files. This is similar to RAG but can be more direct, where the entire content of a file is loaded into the context to be summarized, analyzed, or transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files & Artifacts Example ---\n",
      "The system must support up to 100 concurrent users.\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a dummy file to act as our artifact\n",
    "file_content = \"\"\"\n",
    "PROJECT REQUIREMENTS DOCUMENT\n",
    "1. The user interface must be blue.\n",
    "2. The login button must be labeled 'Enter'.\n",
    "3. The system must support up to 100 concurrent users.\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(file_content)\n",
    "\n",
    "# Now, load the file content\n",
    "with open(\"requirements.txt\", \"r\") as f:\n",
    "    file_context = f.read()\n",
    "\n",
    "file_prompt_template = \"\"\"\n",
    "You are a project manager assistant. Answer the user's question based on the following project requirements file.\n",
    "\n",
    "--- File Content: requirements.txt ---\n",
    "{file_context}\n",
    "--- End File Content ---\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "\n",
    "file_prompt = ChatPromptTemplate.from_template(file_prompt_template)\n",
    "\n",
    "file_chain = file_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"--- Files & Artifacts Example ---\")\n",
    "response = file_chain.invoke({\n",
    "    \"file_context\": file_context,\n",
    "    \"user_question\": \"How many users must the system support?\"\n",
    "})\n",
    "print(response)\n",
    "\n",
    "# Clean up the dummy file\n",
    "os.remove(\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Context engineering is a comprehensive discipline that involves strategically managing different types of information to build powerful and accurate AI agents. By moving beyond simple RAG, you can control an agent's persona, grant it new abilities with tools, give it a memory, and allow it to interact with files.\n",
    "\n",
    "Mastering these techniques is the key to unlocking the full potential of Large Language Models. Keep experimenting! üõ†Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
