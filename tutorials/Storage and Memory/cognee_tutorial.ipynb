{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognee Tutorial: Building AI Agent Memory\n",
    "\n",
    "Welcome to this tutorial on **Cognee**, an open-source AI memory layer that helps you build more intelligent and context-aware AI agents. Cognee enhances the accuracy of AI applications by transforming unstructured data into a structured **knowledge graph**. This allows your AI agents to have a persistent memory, reducing hallucinations and providing more accurate responses.\n",
    "\n",
    "In this notebook, we will cover the fundamental concepts of Cognee and walk you through the process of installing it, ingesting data, creating a knowledge graph, and querying it to build a simple conversational agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the Cognee library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cognee -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API Keys from .env\n",
    "\n",
    "Cognee uses Large Language Models (LLMs) and embedding models to process data. You'll need to provide API keys for these services. For this tutorial, we'll use OpenAI. It's a best practice to store your API keys in a `.env` file to keep them secure.\n",
    "\n",
    "1.  Create a file named `.env` in the same directory as this notebook.\n",
    "2.  Add your OpenAI API key to the `.env` file as follows:\n",
    "\n",
    "```\n",
    "LLM_API_KEY=\"your_openai_api_key\"\n",
    "```\n",
    "\n",
    "Now, let's load the environment variables from the `.env` file. We'll use the `dotenv` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLM_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Concepts of Cognee\n",
    "\n",
    "Cognee's architecture is built around a few key concepts:\n",
    "\n",
    "* **ECL (Extract, Cognify, Load) Pipelines:** These are scalable and modular pipelines for building agent memory.\n",
    "* **Tasks and Pipelines:** Cognee uses `tasks` to perform specific actions (e.g., chunking text, extracting entities) and chains them together into `pipelines`.\n",
    "* **Dual Storage:** Cognee uses a **graph database** to store explicit relationships between data points and a **vector database** for semantic similarity search.\n",
    "\n",
    "The main functions you'll interact with are:\n",
    "\n",
    "* `cognee.add()`: Ingests data from various sources.\n",
    "* `cognee.cognify()`: Processes the ingested data and builds the knowledge graph.\n",
    "* `cognee.search()`: Queries the knowledge graph to find relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion and Knowledge Graph Creation\n",
    "\n",
    "Let's start by adding some text data to Cognee and then cognifying it to create our knowledge graph. We will use a simple text about the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-08-27T08:08:21.800405\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDeleted old log file: /Users/devon/.pyenv/versions/3.10.14/lib/python3.10/site-packages/logs/2025-08-25_12-06-13.log\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:21.803458\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLogging initialized           \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m \u001b[36mcognee_version\u001b[0m=\u001b[35m0.2.3\u001b[0m \u001b[36mos_info\u001b[0m=\u001b[35m'Darwin 23.6.0 (Darwin Kernel Version 23.6.0: Fri Nov 15 15:13:28 PST 2024; root:xnu-10063.141.1.702.7~1/RELEASE_X86_64)'\u001b[0m \u001b[36mpython_version\u001b[0m=\u001b[35m3.10.14\u001b[0m \u001b[36mstructlog_version\u001b[0m=\u001b[35m25.3.0\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:21.804545\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mWant to learn more? Visit the Cognee documentation: https://docs.cognee.ai\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:28.825719\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSQLite path: /Users/devon/.pyenv/versions/3.10.14/lib/python3.10/site-packages/cognee/.cognee_system/databases\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:28.827770\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVector database path: /Users/devon/.pyenv/versions/3.10.14/lib/python3.10/site-packages/cognee/.cognee_system/databases/cognee.lancedb\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:28.829707\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph database path: /Users/devon/.pyenv/versions/3.10.14/lib/python3.10/site-packages/cognee/.cognee_system/databases/cognee_graph_kuzu\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\u001b[92m16:08:33 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "\n",
      "\u001b[1m\n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\u001b[0m\n",
      "\n",
      "\u001b[1mEmbeddingRateLimiter initialized: enabled=False, requests_limit=60, interval_seconds=60\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:35.711734\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mOntology file 'None' not found. No owl ontology will be attached to the graph.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph created successfully!\n"
     ]
    }
   ],
   "source": [
    "import cognee\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # Add data to Cognee\n",
    "    await cognee.add(\"The Solar System is the gravitationally bound system of the Sun and the objects that orbit it. It was formed 4.6 billion years ago from the gravitational collapse of a giant interstellar molecular cloud. The vast majority (99.86%) of the system's mass is in the Sun, with most of the remaining mass contained in the planet Jupiter. The four inner terrestrial planets—Mercury, Venus, Earth, and Mars—are composed primarily of rock and metal. The four giant outer planets are substantially more massive than the terrestrials. The two largest, Jupiter and Saturn, are gas giants, being composed mainly of hydrogen and helium; the two outermost planets, Uranus and Neptune, are ice giants, being composed mostly of substances with relatively high melting points compared with hydrogen and helium, such as water, ammonia, and methane.\")\n",
    "\n",
    "    # Create the knowledge graph\n",
    "    await cognee.cognify()\n",
    "\n",
    "    print(\"Knowledge graph created successfully!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Jupyter notebooks have their own event loop, so we need to use this workaround\n",
    "    # to run async code.\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying the Knowledge Graph\n",
    "\n",
    "Now that we have a knowledge graph, we can ask questions and get answers from it. Let's ask a few questions about the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-08-27T08:08:40.668815\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph projection completed: 27 nodes, 47 edges in 0.04s\u001b[0m [\u001b[0m\u001b[1m\u001b[34mCogneeGraph\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-08-27T08:08:42.195834\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVector collection retrieval completed: Retrieved distances from 6 collections in 0.03s\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\u001b[92m16:08:42 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "\n",
      "\u001b[1m\n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The four inner planets are Mercury, Venus, Earth, and Mars.\n"
     ]
    }
   ],
   "source": [
    "import cognee\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # Query the knowledge graph\n",
    "    results = await cognee.search(\"What are the four inner planets?\")\n",
    "\n",
    "    # Display the results\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Simple Conversational Agent\n",
    "\n",
    "Now, let's use Cognee to build a simple conversational agent that can answer questions based on the knowledge it has. We can create a loop to ask multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognee\n",
    "import asyncio\n",
    "\n",
    "async def chat():\n",
    "    print(\"Hello! I am a conversational agent with knowledge about the solar system. Ask me a question or type 'quit' to exit.\")\n",
    "    while True:\n",
    "        query = input(\"> \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        results = await cognee.search(query)\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    # To run the chat in a notebook, you might want to call it directly\n",
    "    # instead of using asyncio.run() to avoid blocking issues.\n",
    "    # However, for simplicity, we will run it once.\n",
    "    # For a real chatbot, you'd integrate this into an application.\n",
    "    print(\"Starting chat... (run the cell to ask a question)\")\n",
    "    # This is a simplified example. In a real application, you would handle the input loop differently.\n",
    "    # For this notebook, we'll just ask one question.\n",
    "    asyncio.run(chat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cognee1](/products/cognee1.png)\n",
    "![Cognee2](/products/cognee2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a simple AI agent with memory using Cognee. You've learned how to:\n",
    "\n",
    "* Install and set up Cognee.\n",
    "* Securely load API keys from a `.env` file.\n",
    "* Ingest data and create a knowledge graph.\n",
    "* Query the knowledge graph to answer questions.\n",
    "\n",
    "This is just the beginning of what you can do with Cognee. You can explore more advanced features like ingesting data from various sources (PDFs, websites, etc.), building custom pipelines, and integrating Cognee with other AI frameworks like LangChain and LlamaIndex.\n",
    "\n",
    "For more information, check out the official Cognee documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
