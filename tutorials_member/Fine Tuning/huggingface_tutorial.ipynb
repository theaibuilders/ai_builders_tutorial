{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face: From Zero to Fine-tuning\n",
    "\n",
    "Welcome to this comprehensive yet concise tutorial on Hugging Face! By the end of this notebook, you'll have a solid understanding of the core concepts and be able to use the Hugging Face ecosystem for your own NLP projects. We'll cover the essentials, from the `pipeline` function for quick and easy inference to loading models and tokenizers for more custom tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install necessary libraries\n",
    "# !pip install transformers datasets torch torchvision torchaudio python-dotenv numpy==1.26.4 accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_ACCESS_TOKEN')\n",
    "\n",
    "if not hf_token:\n",
    "    raise ValueError('HUGGINGFACE_ACCESS_TOKEN not found in .env file')\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The `pipeline` Function: Your Gateway to NLP\n",
    "\n",
    "The `pipeline` function is the easiest way to get started with Hugging Face. It abstracts away most of the complexity and allows you to perform a wide range of tasks with just a few lines of code. Let's explore some of the most common tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998737573623657}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "result = sentiment_analyzer(\"Hugging Face is awesome!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where AI is becoming more prevalent, AI is also evolving into a whole new field, and we are seeing a lot of AI evolving into a whole new field.\n",
      "\n",
      "What's your takeaway from this?\n",
      "\n",
      "I think that as AI grows, we will see a lot more of it. The more we learn about the problem of AI, the more we will realize how important it is to learn about AI. That's a good thing because there may be more of it.\n",
      "\n",
      "If you can't get it to become a part of your life, then you're not really there.\n",
      "\n",
      "Do you think it's possible that AI will be able to do more to drive the next AI breakthrough?\n",
      "\n",
      "If the AI breakthroughs are to happen, I'm not sure that we will.\n",
      "\n",
      "\"The future is always being explored and developed\"\n",
      "\n",
      "Yes, yes it is.\n",
      "\n",
      "How do you think the AI revolution has changed the way we think about AI?\n",
      "\n",
      "I think that's a good question. I think that the AI revolution has changed the way we think about AI.\n",
      "\n",
      "The future is always being explored and developed.\n",
      "\n",
      "If you look at the way human beings have been doing this for a long time, and we're slowly making that transition, then\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)  # Suppress transformers logs\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "result = text_generator(\"In a world where AI is becoming more prevalent,\", max_length=50)\n",
    "\n",
    "# Only print the generated text\n",
    "if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
    "    print(result[0]['generated_text'])\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56aa310c1f04281b0e9f4bbfa87c154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b438cc5cb26e42deaad6d3121bb19293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1730c930498948649a1ec3fb9ce9cb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02659ebb0f7446009cdf2981e561165e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9970439, 'word': 'John Doe', 'start': 11, 'end': 19}, {'entity_group': 'LOC', 'score': 0.9993253, 'word': 'New York City', 'start': 34, 'end': 47}]\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner_pipeline(\"My name is John Doe and I live in New York City.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Under the Hood: Models and Tokenizers\n",
    "\n",
    "While the `pipeline` function is great for quick tasks, you'll often need more control over the model and tokenizer. Let's see how to load them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input\n",
    "text = \"Hugging Face is a great tool for NLP.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Make the prediction\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = logits.argmax().item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# Example with negative sentiment (should predict class 0)\n",
    "text_neg = \"This is a terrible experience. I am very disappointed.\"\n",
    "inputs_neg = tokenizer(text_neg, return_tensors=\"pt\")\n",
    "outputs_neg = model(**inputs_neg)\n",
    "logits_neg = outputs_neg.logits\n",
    "predicted_class_neg = logits_neg.argmax().item()\n",
    "print(f\"Predicted class: {predicted_class_neg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output Predicted class: 1 means that the model classified your input text (\"Hugging Face is a great tool for NLP.\") as belonging to class 1. In most sentiment analysis models like distilbert-base-uncased-finetuned-sst-2-english, class 1 usually represents \"positive\" sentiment, while class 0 represents \"negative\" sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning a Model\n",
    "\n",
    "Fine-tuning allows you to adapt a pre-trained model to your specific dataset. Here's a conceptual overview of the process. For a hands-on example, we'll use the `datasets` library to load a dataset and then fine-tune a model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 44.6356, 'train_samples_per_second': 5.601, 'train_steps_per_second': 0.358, 'train_loss': 0.6679916381835938, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=0.6679916381835938, metrics={'train_runtime': 44.6356, 'train_samples_per_second': 5.601, 'train_steps_per_second': 0.358, 'train_loss': 0.6679916381835938, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)  # Suppress transformers logs\n",
    "logging.getLogger('datasets').setLevel(logging.ERROR)  # Suppress datasets logs\n",
    "logging.getLogger('filelock').setLevel(logging.ERROR)  # Suppress filelock logs\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1%]\")\n",
    "\n",
    "# Load a tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Congratulations! ðŸŽ‰ You've now learned the fundamentals of Hugging Face. You can use the `pipeline` for quick inference, load models and tokenizers for more control, and even fine-tune pre-trained models on your own data. This is just the beginning of your journey with Hugging Face. Explore the Hugging Face Hub website to discover thousands of models and datasets, and dive deeper into the documentation to unlock the full power of the ecosystem.\n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
